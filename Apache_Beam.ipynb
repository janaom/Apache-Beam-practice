{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoVN6pE2/dKinQBve7g0Xi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janaom/Apache_Beam_practice/blob/main/Apache_Beam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hrd5nv2RAn27"
      },
      "outputs": [],
      "source": [
        "!pip install apache-beam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install apache-beam[interactive]"
      ],
      "metadata": {
        "id": "qnSAQ5mjAcHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploade = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "O9F0QUjHBP7g",
        "outputId": "c5e8bd47-0623-4c00-9169-1c8c4fdea312"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-61a844a3-157c-4304-bdf1-07a1388a2a29\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-61a844a3-157c-4304-bdf1-07a1388a2a29\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving muestra.txt to muestra.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "p2 = beam.Pipeline()\n",
        "\n",
        "lines = (\n",
        "            p2\n",
        "            | beam.Create([\n",
        "               'Using create transform ',\n",
        "               'to generate in memory data ',\n",
        "               'This is 3rd line ',\n",
        "               'Thanks '])\n",
        "\n",
        "            | beam.io.WriteToText('data/outCreate1')\n",
        "          )\n",
        "p2.run()\n",
        "\n",
        "# visualize output\n",
        "!{('head -n 20 data/outCreate1-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eW5oubN-g_j",
        "outputId": "517fb3d5-5ab2-4c69-9b70-8e29eaaf6ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using create transform \n",
            "to generate in memory data \n",
            "This is 3rd line \n",
            "Thanks \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "p3 = beam.Pipeline()\n",
        "\n",
        "lines1 = (p3\n",
        "           #create a list\n",
        "           | beam.Create([1,2,3,4,5,6,7,8,9])\n",
        "\n",
        "           | beam.io.WriteToText('data/outCreate2')\n",
        "          )\n",
        "p3.run()\n",
        "\n",
        "# visualize output\n",
        "!{('head -n 20 data/outCreate2-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSogqfSqA6Tz",
        "outputId": "a7e6ace5-10f0-4707-fe75-3042357ad05e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "p4 = beam.Pipeline()\n",
        "\n",
        "\n",
        "lines = (p4\n",
        "           | beam.Create([(\"maths\",52),(\"english\",75),(\"science\",82), (\"computer\",65),(\"maths\",85)])\n",
        "\n",
        "            | beam.io.WriteToText('data/outCreate3')\n",
        "          )\n",
        "p4.run()\n",
        "\n",
        "# visualize output\n",
        "!{('head -n 20 data/outCreate3-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjq_4O42BorA",
        "outputId": "16854d52-7e99-4ea7-9055-5e60126203f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('maths', 52)\n",
            "('english', 75)\n",
            "('science', 82)\n",
            "('computer', 65)\n",
            "('maths', 85)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "p5 = beam.Pipeline()\n",
        "\n",
        "lines = ( p5\n",
        "\n",
        "       | beam.Create({'row1':[1,2,3,4,5],\n",
        "                     'row2':[1,2,3,4,5]})\n",
        "       | beam.Map(lambda element: element)\n",
        "       | beam.io.WriteToText('data/outCreate4')\n",
        "  )\n",
        "\n",
        "p5.run()\n",
        "\n",
        "# visualize output\n",
        "!{('head -n 20 data/outCreate4-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDDtFaSSB9mb",
        "outputId": "0002df4a-fc9b-4c80-e36c-2622000d9f0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('row1', [1, 2, 3, 4, 5])\n",
            "('row2', [1, 2, 3, 4, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "def SplitRow(element):\n",
        "    return element.split(',')\n",
        "\n",
        "def filtering(record):\n",
        "  return record[3] == 'Accounts'\n",
        "\n",
        "\n",
        "p1 = beam.Pipeline()\n",
        "\n",
        "attendance_count = (\n",
        "\n",
        "   p1\n",
        "    |beam.io.ReadFromText('dept_data.txt')\n",
        "    |beam.Map(SplitRow)\n",
        "   #| beam.Map(lambda record: record.split(','))\n",
        "\n",
        "    |beam.Filter(filtering)\n",
        "  # |beam.Filter(lambda record: record[3] == 'Accounts')\n",
        "\n",
        "    |beam.Map(lambda record: (record[1], 1))\n",
        "    |beam.CombinePerKey(sum)\n",
        "\n",
        "    |beam.io.WriteToText('data/output_new_final')\n",
        "\n",
        ")\n",
        "\n",
        "p1.run()\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/output_new_final-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l96l4eVsEYN6",
        "outputId": "1906a651-368e-44b5-f035-c41daabb2375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Marco', 31)\n",
            "('Rebekah', 31)\n",
            "('Itoe', 31)\n",
            "('Edouard', 31)\n",
            "('Kyle', 62)\n",
            "('Kumiko', 31)\n",
            "('Gaston', 31)\n",
            "('Ayumi', 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "\n",
        "p1 = beam.Pipeline()\n",
        "\n",
        "attendance_count = (\n",
        "\n",
        "   p1\n",
        "    |beam.io.ReadFromText('dept_data.txt')\n",
        "    |beam.io.WriteToText('data/output_new_final')\n",
        "\n",
        ")\n",
        "\n",
        "p1.run()\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/output_new_final-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "CQoaZjNBH5B7",
        "outputId": "ae74777c-f85c-48e7-eb8b-96deea846a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
              "          var jqueryScript = document.createElement('script');\n",
              "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
              "          jqueryScript.type = 'text/javascript';\n",
              "          jqueryScript.onload = function() {\n",
              "            var datatableScript = document.createElement('script');\n",
              "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
              "            datatableScript.type = 'text/javascript';\n",
              "            datatableScript.onload = function() {\n",
              "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
              "              window.interactive_beam_jquery(document).ready(function($){\n",
              "                \n",
              "              });\n",
              "            }\n",
              "            document.head.appendChild(datatableScript);\n",
              "          };\n",
              "          document.head.appendChild(jqueryScript);\n",
              "        } else {\n",
              "          window.interactive_beam_jquery(document).ready(function($){\n",
              "            \n",
              "          });\n",
              "        }"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "149633CM,Marco,10,Accounts,1-01-2019\n",
            "212539MU,Rebekah,10,Accounts,1-01-2019\n",
            "231555ZZ,Itoe,10,Accounts,1-01-2019\n",
            "503996WI,Edouard,10,Accounts,1-01-2019\n",
            "704275DC,Kyle,10,Accounts,1-01-2019\n",
            "957149WC,Kyle,10,Accounts,1-01-2019\n",
            "241316NX,Kumiko,10,Accounts,1-01-2019\n",
            "796656IE,Gaston,10,Accounts,1-01-2019\n",
            "331593PS,Beryl,20,HR,1-01-2019\n",
            "560447WH,Olga,20,HR,1-01-2019\n",
            "222997TJ,Leslie,20,HR,1-01-2019\n",
            "171752SY,Mindy,20,HR,1-01-2019\n",
            "153636AS,Vicky,20,HR,1-01-2019\n",
            "745411HT,Richard,20,HR,1-01-2019\n",
            "298464HN,Kirk,20,HR,1-01-2019\n",
            "783950BW,Kaori,20,HR,1-01-2019\n",
            "892691AR,Beryl,20,HR,1-01-2019\n",
            "245668UZ,Oscar,20,HR,1-01-2019\n",
            "231206QD,Kumiko,30,Finance,1-01-2019\n",
            "357919KT,Wendy,30,Finance,1-01-2019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "def SplitRow(element):\n",
        "    return element.split(',')\n",
        "\n",
        "\n",
        "p1 = beam.Pipeline()\n",
        "\n",
        "attendance_count = (\n",
        "\n",
        "   p1\n",
        "    |beam.io.ReadFromText('dept_data.txt')\n",
        "    |beam.Map(SplitRow)          #takes one string/element '149633CM,Marco,10,Accounts,1-01-2019' and divide with commas with def SplitRow\n",
        "    |beam.io.WriteToText('data/output_new_final')\n",
        ")\n",
        "\n",
        "p1.run()\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/output_new_final-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHZxN4qaLrTp",
        "outputId": "6e57d3ba-f9e0-4e98-f056-c08e735a007b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['149633CM', 'Marco', '10', 'Accounts', '1-01-2019']\n",
            "['212539MU', 'Rebekah', '10', 'Accounts', '1-01-2019']\n",
            "['231555ZZ', 'Itoe', '10', 'Accounts', '1-01-2019']\n",
            "['503996WI', 'Edouard', '10', 'Accounts', '1-01-2019']\n",
            "['704275DC', 'Kyle', '10', 'Accounts', '1-01-2019']\n",
            "['957149WC', 'Kyle', '10', 'Accounts', '1-01-2019']\n",
            "['241316NX', 'Kumiko', '10', 'Accounts', '1-01-2019']\n",
            "['796656IE', 'Gaston', '10', 'Accounts', '1-01-2019']\n",
            "['331593PS', 'Beryl', '20', 'HR', '1-01-2019']\n",
            "['560447WH', 'Olga', '20', 'HR', '1-01-2019']\n",
            "['222997TJ', 'Leslie', '20', 'HR', '1-01-2019']\n",
            "['171752SY', 'Mindy', '20', 'HR', '1-01-2019']\n",
            "['153636AS', 'Vicky', '20', 'HR', '1-01-2019']\n",
            "['745411HT', 'Richard', '20', 'HR', '1-01-2019']\n",
            "['298464HN', 'Kirk', '20', 'HR', '1-01-2019']\n",
            "['783950BW', 'Kaori', '20', 'HR', '1-01-2019']\n",
            "['892691AR', 'Beryl', '20', 'HR', '1-01-2019']\n",
            "['245668UZ', 'Oscar', '20', 'HR', '1-01-2019']\n",
            "['231206QD', 'Kumiko', '30', 'Finance', '1-01-2019']\n",
            "['357919KT', 'Wendy', '30', 'Finance', '1-01-2019']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "def SplitRow(element):\n",
        "    return element.split(',')\n",
        "\n",
        "\n",
        "p1 = beam.Pipeline()\n",
        "\n",
        "attendance_count = (\n",
        "\n",
        "   p1\n",
        "    |beam.io.ReadFromText('dept_data.txt')\n",
        "    |beam.FlatMap(SplitRow)\n",
        "    |beam.io.WriteToText('data/output_new_final')\n",
        ")\n",
        "\n",
        "p1.run()\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/output_new_final-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmPWiOu-NNCF",
        "outputId": "d4755f9e-f30f-4392-cbd0-b49d7c6cdf08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "149633CM\n",
            "Marco\n",
            "10\n",
            "Accounts\n",
            "1-01-2019\n",
            "212539MU\n",
            "Rebekah\n",
            "10\n",
            "Accounts\n",
            "1-01-2019\n",
            "231555ZZ\n",
            "Itoe\n",
            "10\n",
            "Accounts\n",
            "1-01-2019\n",
            "503996WI\n",
            "Edouard\n",
            "10\n",
            "Accounts\n",
            "1-01-2019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "def SplitRow(element):\n",
        "    return [element.split(',')]\n",
        "\n",
        "\n",
        "p1 = beam.Pipeline()\n",
        "\n",
        "attendance_count = (\n",
        "\n",
        "   p1\n",
        "    |beam.io.ReadFromText('dept_data.txt')\n",
        "    |beam.FlatMap(SplitRow)\n",
        "    |beam.io.WriteToText('data/output_new_final')\n",
        ")\n",
        "\n",
        "p1.run()\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/output_new_final-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sycoY0T-Np-1",
        "outputId": "fc14429d-e3de-4aa8-ff50-a24fd0e44d69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['149633CM', 'Marco', '10', 'Accounts', '1-01-2019']\n",
            "['212539MU', 'Rebekah', '10', 'Accounts', '1-01-2019']\n",
            "['231555ZZ', 'Itoe', '10', 'Accounts', '1-01-2019']\n",
            "['503996WI', 'Edouard', '10', 'Accounts', '1-01-2019']\n",
            "['704275DC', 'Kyle', '10', 'Accounts', '1-01-2019']\n",
            "['957149WC', 'Kyle', '10', 'Accounts', '1-01-2019']\n",
            "['241316NX', 'Kumiko', '10', 'Accounts', '1-01-2019']\n",
            "['796656IE', 'Gaston', '10', 'Accounts', '1-01-2019']\n",
            "['331593PS', 'Beryl', '20', 'HR', '1-01-2019']\n",
            "['560447WH', 'Olga', '20', 'HR', '1-01-2019']\n",
            "['222997TJ', 'Leslie', '20', 'HR', '1-01-2019']\n",
            "['171752SY', 'Mindy', '20', 'HR', '1-01-2019']\n",
            "['153636AS', 'Vicky', '20', 'HR', '1-01-2019']\n",
            "['745411HT', 'Richard', '20', 'HR', '1-01-2019']\n",
            "['298464HN', 'Kirk', '20', 'HR', '1-01-2019']\n",
            "['783950BW', 'Kaori', '20', 'HR', '1-01-2019']\n",
            "['892691AR', 'Beryl', '20', 'HR', '1-01-2019']\n",
            "['245668UZ', 'Oscar', '20', 'HR', '1-01-2019']\n",
            "['231206QD', 'Kumiko', '30', 'Finance', '1-01-2019']\n",
            "['357919KT', 'Wendy', '30', 'Finance', '1-01-2019']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "def SplitRow(element):\n",
        "    return [element.split(',')]\n",
        "\n",
        "\n",
        "p1 = beam.Pipeline()\n",
        "\n",
        "attendance_count = (\n",
        "\n",
        "   p1\n",
        "    |beam.io.ReadFromText('dept_data.txt')\n",
        "    |beam.Map(SplitRow)\n",
        "    |beam.io.WriteToText('data/output_new_final')\n",
        ")\n",
        "\n",
        "p1.run()\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/output_new_final-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aj1ENiwXOKRJ",
        "outputId": "70552734-1d77-4ea6-d2bd-582998a23d34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['149633CM', 'Marco', '10', 'Accounts', '1-01-2019']]\n",
            "[['212539MU', 'Rebekah', '10', 'Accounts', '1-01-2019']]\n",
            "[['231555ZZ', 'Itoe', '10', 'Accounts', '1-01-2019']]\n",
            "[['503996WI', 'Edouard', '10', 'Accounts', '1-01-2019']]\n",
            "[['704275DC', 'Kyle', '10', 'Accounts', '1-01-2019']]\n",
            "[['957149WC', 'Kyle', '10', 'Accounts', '1-01-2019']]\n",
            "[['241316NX', 'Kumiko', '10', 'Accounts', '1-01-2019']]\n",
            "[['796656IE', 'Gaston', '10', 'Accounts', '1-01-2019']]\n",
            "[['331593PS', 'Beryl', '20', 'HR', '1-01-2019']]\n",
            "[['560447WH', 'Olga', '20', 'HR', '1-01-2019']]\n",
            "[['222997TJ', 'Leslie', '20', 'HR', '1-01-2019']]\n",
            "[['171752SY', 'Mindy', '20', 'HR', '1-01-2019']]\n",
            "[['153636AS', 'Vicky', '20', 'HR', '1-01-2019']]\n",
            "[['745411HT', 'Richard', '20', 'HR', '1-01-2019']]\n",
            "[['298464HN', 'Kirk', '20', 'HR', '1-01-2019']]\n",
            "[['783950BW', 'Kaori', '20', 'HR', '1-01-2019']]\n",
            "[['892691AR', 'Beryl', '20', 'HR', '1-01-2019']]\n",
            "[['245668UZ', 'Oscar', '20', 'HR', '1-01-2019']]\n",
            "[['231206QD', 'Kumiko', '30', 'Finance', '1-01-2019']]\n",
            "[['357919KT', 'Wendy', '30', 'Finance', '1-01-2019']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "def SplitRow(element):\n",
        "    return element.split(',')\n",
        "\n",
        "def filtering(record):\n",
        "  return record[3] == 'Accounts'\n",
        "\n",
        "\n",
        "p1 = beam.Pipeline()\n",
        "\n",
        "attendance_count = (\n",
        "\n",
        "   p1\n",
        "    |beam.io.ReadFromText('dept_data.txt')\n",
        "    |beam.Map(SplitRow)\n",
        "   #| beam.Map(lambda record: record.split(','))\n",
        "\n",
        "    |beam.Filter(filtering)\n",
        "  # |beam.Filter(lambda record: record[3] == 'Accounts')\n",
        "\n",
        "\n",
        "    |beam.io.WriteToText('data/output_new_final')\n",
        "\n",
        ")\n",
        "\n",
        "p1.run()\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/output_new_final-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GVAxmAAOt_g",
        "outputId": "4f326010-3580-4498-8095-87e4badc7707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['149633CM', 'Marco', '10', 'Accounts', '1-01-2019']\n",
            "['212539MU', 'Rebekah', '10', 'Accounts', '1-01-2019']\n",
            "['231555ZZ', 'Itoe', '10', 'Accounts', '1-01-2019']\n",
            "['503996WI', 'Edouard', '10', 'Accounts', '1-01-2019']\n",
            "['704275DC', 'Kyle', '10', 'Accounts', '1-01-2019']\n",
            "['957149WC', 'Kyle', '10', 'Accounts', '1-01-2019']\n",
            "['241316NX', 'Kumiko', '10', 'Accounts', '1-01-2019']\n",
            "['796656IE', 'Gaston', '10', 'Accounts', '1-01-2019']\n",
            "['149633CM', 'Marco', '10', 'Accounts', '2-01-2019']\n",
            "['212539MU', 'Rebekah', '10', 'Accounts', '2-01-2019']\n",
            "['231555ZZ', 'Itoe', '10', 'Accounts', '2-01-2019']\n",
            "['503996WI', 'Edouard', '10', 'Accounts', '2-01-2019']\n",
            "['704275DC', 'Kyle', '10', 'Accounts', '2-01-2019']\n",
            "['957149WC', 'Kyle', '10', 'Accounts', '2-01-2019']\n",
            "['241316NX', 'Kumiko', '10', 'Accounts', '2-01-2019']\n",
            "['796656IE', 'Gaston', '10', 'Accounts', '2-01-2019']\n",
            "['718737IX', 'Ayumi', '10', 'Accounts', '2-01-2019']\n",
            "['149633CM', 'Marco', '10', 'Accounts', '3-01-2019']\n",
            "['212539MU', 'Rebekah', '10', 'Accounts', '3-01-2019']\n",
            "['231555ZZ', 'Itoe', '10', 'Accounts', '3-01-2019']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "def SplitRow(element):\n",
        "    return element.split(',')\n",
        "\n",
        "def filtering(record):\n",
        "  return record[3] == 'Accounts'\n",
        "\n",
        "\n",
        "p1 = beam.Pipeline()\n",
        "\n",
        "attendance_count = (\n",
        "\n",
        "   p1\n",
        "    |beam.io.ReadFromText('dept_data.txt')\n",
        "    |beam.Map(SplitRow)\n",
        "   #| beam.Map(lambda record: record.split(','))\n",
        "\n",
        "    |beam.Filter(filtering)\n",
        "  # |beam.Filter(lambda record: record[3] == 'Accounts')\n",
        "\n",
        "    |beam.Map(lambda record: (record[1], 1))\n",
        "    |beam.CombinePerKey(sum)\n",
        "\n",
        "    |beam.io.WriteToText('data/output_new_final')\n",
        "\n",
        ")\n",
        "\n",
        "p1.run()\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/output_new_final-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJRUPA5JJkTe",
        "outputId": "60812770-0ac7-469f-fb06-b33d9501bef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Marco', 31)\n",
            "('Rebekah', 31)\n",
            "('Itoe', 31)\n",
            "('Edouard', 31)\n",
            "('Kyle', 62)\n",
            "('Kumiko', 31)\n",
            "('Gaston', 31)\n",
            "('Ayumi', 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "p1 = beam.Pipeline()\n",
        "\n",
        "attendance_count = (\n",
        "\n",
        "   p1\n",
        "    |'Read from file' >> beam.io.ReadFromText('dept_data.txt')  #with Read from file we will lable our transform so it'll be easy to debug\n",
        "    |'Map transform' >> beam.Map(lambda record: record.split(','))\n",
        "    |beam.Filter(lambda record: record[3] == 'Accounts')\n",
        "    |beam.Map(lambda record: (record[1], 1))\n",
        "    |beam.CombinePerKey(sum)\n",
        "    |beam.io.WriteToText('data/output_new_final')\n",
        "\n",
        ")\n",
        "\n",
        "p1.run()\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/output_new_final-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "LusHL6122vk3",
        "outputId": "12b2ecb4-7408-45d8-ee9e-2157497b0c53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
              "          var jqueryScript = document.createElement('script');\n",
              "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
              "          jqueryScript.type = 'text/javascript';\n",
              "          jqueryScript.onload = function() {\n",
              "            var datatableScript = document.createElement('script');\n",
              "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
              "            datatableScript.type = 'text/javascript';\n",
              "            datatableScript.onload = function() {\n",
              "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
              "              window.interactive_beam_jquery(document).ready(function($){\n",
              "                \n",
              "              });\n",
              "            }\n",
              "            document.head.appendChild(datatableScript);\n",
              "          };\n",
              "          document.head.appendChild(jqueryScript);\n",
              "        } else {\n",
              "          window.interactive_beam_jquery(document).ready(function($){\n",
              "            \n",
              "          });\n",
              "        }"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Marco', 31)\n",
            "('Rebekah', 31)\n",
            "('Itoe', 31)\n",
            "('Edouard', 31)\n",
            "('Kyle', 62)\n",
            "('Kumiko', 31)\n",
            "('Gaston', 31)\n",
            "('Ayumi', 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "with beam.Pipeline() as p1:\n",
        "\n",
        "  attenance_count = (\n",
        "\n",
        "   p1\n",
        "    |'Read from file' >> beam.io.ReadFromText('dept_data.txt')  #with Read from file we will lable our transform so it'll be easy to debug\n",
        "    |'Map transform' >> beam.Map(lambda record: record.split(','))\n",
        "    |beam.Filter(lambda record: record[3] == 'Accounts')\n",
        "    |beam.Map(lambda record: (record[1], 1))\n",
        "    |beam.CombinePerKey(sum)\n",
        "    |beam.io.WriteToText('data/output_new_final')\n",
        "\n",
        ")\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/output_new_final-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnjDEsoc3mYL",
        "outputId": "74fb4966-de32-4100-9ac2-6f2d9030cd4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Marco', 31)\n",
            "('Rebekah', 31)\n",
            "('Itoe', 31)\n",
            "('Edouard', 31)\n",
            "('Kyle', 62)\n",
            "('Kumiko', 31)\n",
            "('Gaston', 31)\n",
            "('Ayumi', 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "def SplitRow(element):\n",
        "    return element.split(',')\n",
        "\n",
        "p = beam.Pipeline()\n",
        "\n",
        "\n",
        "input_collection = (\n",
        "                      p\n",
        "                      | \"Read from text file\" >> beam.io.ReadFromText('dept_data.txt')\n",
        "                      | \"Split rows\" >> beam.Map(SplitRow)\n",
        "                   )\n",
        "\n",
        "accounts_count = (\n",
        "                      input_collection\n",
        "                      | 'Get all Accounts dept persons' >> beam.Filter(lambda record: record[3] == 'Accounts')\n",
        "                      | 'Pair each accounts employee with 1' >> beam.Map(lambda record: (\"Accounts, \" +record[1], 1))\n",
        "                      | 'Group and sum1' >> beam.CombinePerKey(sum)\n",
        "                    #  | 'Write results for account' >> beam.io.WriteToText('data/Account')\n",
        "                 )\n",
        "\n",
        "hr_count = (\n",
        "                input_collection\n",
        "                | 'Get all HR dept persons' >> beam.Filter(lambda record: record[3] == 'HR')\n",
        "                | 'Pair each hr employee with 1' >> beam.Map(lambda record: (\"HR, \" +record[1], 1))\n",
        "                | 'Group and sum' >> beam.CombinePerKey(sum)\n",
        "                #| 'Write results for hr' >> beam.io.WriteToText('data/HR')\n",
        "           )\n",
        "\n",
        "output =(\n",
        "         (accounts_count,hr_count)\n",
        "    | beam.Flatten()\n",
        "    | beam.io.WriteToText('data/both')\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "p.run()\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/both-00000-of-00001')}\n",
        "\n",
        "\n",
        "#!{('head -n 20 data/HR-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdKtPdUj-5Mf",
        "outputId": "2b65de59-0687-4a05-aacf-b99a89637416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Accounts, Marco', 31)\n",
            "('Accounts, Rebekah', 31)\n",
            "('Accounts, Itoe', 31)\n",
            "('Accounts, Edouard', 31)\n",
            "('Accounts, Kyle', 62)\n",
            "('Accounts, Kumiko', 31)\n",
            "('Accounts, Gaston', 31)\n",
            "('Accounts, Ayumi', 30)\n",
            "('HR, Beryl', 62)\n",
            "('HR, Olga', 31)\n",
            "('HR, Leslie', 31)\n",
            "('HR, Mindy', 31)\n",
            "('HR, Vicky', 31)\n",
            "('HR, Richard', 31)\n",
            "('HR, Kirk', 31)\n",
            "('HR, Kaori', 31)\n",
            "('HR, Oscar', 31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "from typing import List, Tuple\n",
        "\n",
        "import apache_beam as beam\n",
        "from apache_beam import PCollection\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--input-location\", required=True)\n",
        "    parser.add_argument(\"--output-location\", required=True)\n",
        "    parser.add_argument(\"--num-of-words\", default=1000, required=False)\n",
        "\n",
        "    my_args, beam_args = parser.parse_known_args()\n",
        "    run_pipeline(my_args, beam_args)\n",
        "\n",
        "\n",
        "def sanitize_word(word: str) -> str:\n",
        "    word = word.lower()\n",
        "    word = word.replace(\",\", \"\").replace(\".\", \"\")\n",
        "    return word\n",
        "\n",
        "\n",
        "def prettify(tl: List[Tuple[str, int]]) -> str:\n",
        "    pretty_str = \"\"\n",
        "    for t in tl:\n",
        "        pretty_str += f\"{t[0]},{t[1]}\\n\"\n",
        "    return pretty_str\n",
        "\n",
        "\n",
        "def run_pipeline(custom_args, beam_args):\n",
        "    opts = PipelineOptions(beam_args)\n",
        "\n",
        "    input_location = custom_args.input_location\n",
        "    output_location = custom_args.output_location\n",
        "    num_words = custom_args.num_of_words\n",
        "\n",
        "    with beam.pipeline.Pipeline(options=opts) as p:\n",
        "        # Reading the data\n",
        "        lines: PCollection[str] = p | \"Reading input data\" >> beam.io.ReadFromText(file_pattern=input_location)\n",
        "\n",
        "        # \"En un lugar de la Mancha\", \"cuyo nombre...\", \"another sentence\", ...\n",
        "        words: PCollection[str] = lines | \"Split words\" >> beam.FlatMap(lambda line: line.split())\n",
        "\n",
        "        # \"En\", \"un\", \"lugar\", ..., \"en\"\n",
        "        sanitized: PCollection[str] = words | \"Sanitize words\" >> beam.Map(sanitize_word)\n",
        "        # \"en\", \"un\", \"lugar\", ..., \"en\"\n",
        "\n",
        "        counted: PCollection[Tuple[str, int]] = sanitized | \"Count\" >> beam.combiners.Count.PerElement()\n",
        "\n",
        "        ranked: PCollection[List[Tuple[str, int]]] = counted | \"Rank\" >> beam.combiners.Top.Of(num_words,\n",
        "                                                                                               key=lambda t: t[1])\n",
        "\n",
        "        prettied: PCollection[str] = ranked | \"Pretty print\" >> beam.Map(prettify)\n",
        "        prettied | \"Write output\" >> beam.io.WriteToText(output_location)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Run this script in Colab:\n",
        "    #!python main.py --input-location=./data.txt --output-location=./out/words.csv\n",
        "    main()"
      ],
      "metadata": {
        "id": "YXkVJpzoBR93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --input-location=./data.txt --output-location=./out/words.csv"
      ],
      "metadata": {
        "id": "Kk-dUAH0B5gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "\n",
        "# Define a custom transform to split the text into individual words\n",
        "class SplitWords(beam.DoFn):\n",
        "    def process(self, element):\n",
        "        words = element.strip().split()\n",
        "        return words\n",
        "\n",
        "# Create a PipelineOptions object to configure the Beam pipeline\n",
        "options = PipelineOptions()\n",
        "\n",
        "# Create a Pipeline using the PipelineOptions\n",
        "with beam.Pipeline(options=options) as pipeline:\n",
        "    # Read the input text from a file\n",
        "    lines = pipeline | 'ReadText' >> beam.io.ReadFromText('./data.txt')\n",
        "\n",
        "    # Apply transformations to perform word count\n",
        "    word_counts = (\n",
        "        lines\n",
        "        | 'SplitWords' >> beam.ParDo(SplitWords())\n",
        "        | 'CountWords' >> beam.combiners.Count.PerElement()\n",
        "    )\n",
        "\n",
        "    # Sort the word counts in descending order\n",
        "    sorted_word_counts = (\n",
        "        word_counts\n",
        "        | 'SortWordCounts' >> beam.transforms.combiners.Top.Of(5, key=lambda x: x[1])\n",
        "    )\n",
        "\n",
        "    # Write the top five most used words to output\n",
        "    sorted_word_counts | 'WriteOutput' >> beam.Map(print)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZaI3AipDlW2",
        "outputId": "1e1d20b4-0e7c-426d-fdfb-a7c8d408e76d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-8c2e45e4-d496-42d2-9cc4-b14170b743dc.json']\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-8c2e45e4-d496-42d2-9cc4-b14170b743dc.json']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 786), ('I', 611), ('and', 585), ('of', 439), ('to', 423)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "import re\n",
        "\n",
        "inputs_pattern = './data.txt'\n",
        "outputs_prefix = 'outputs/part'\n",
        "\n",
        "# Running locally in the DirectRunner.\n",
        "with beam.Pipeline() as pipeline:\n",
        "  (\n",
        "      pipeline\n",
        "      | 'Read lines' >> beam.io.ReadFromText(inputs_pattern)\n",
        "      | 'Find words' >> beam.FlatMap(lambda line: re.findall(r\"[a-zA-Z']+\", line))\n",
        "      | 'Pair words with 1' >> beam.Map(lambda word: (word, 1))\n",
        "      | 'Group and sum' >> beam.CombinePerKey(sum)\n",
        "      | 'Format results' >> beam.Map(lambda word_count: str(word_count))\n",
        "      | 'Write results' >> beam.io.WriteToText(outputs_prefix)\n",
        "\n",
        "  )\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!#run('head -n 20 {}-00000-of-*'.format(outputs_prefix))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPGQqHY-FjQ7",
        "outputId": "abbd56b9-1186-41b7-b348-ada4b9f9cb4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "import re\n",
        "\n",
        "inputs_pattern = './data.txt'\n",
        "outputs_prefix = 'outputs/part'\n",
        "\n",
        "# Running locally in the DirectRunner.\n",
        "with beam.Pipeline() as pipeline:\n",
        "    (\n",
        "        pipeline\n",
        "        | 'Read lines' >> beam.io.ReadFromText(inputs_pattern)\n",
        "        | 'Find words' >> beam.FlatMap(lambda line: re.findall(r\"[a-zA-Z']+\", line))\n",
        "        | 'Pair words with 1' >> beam.Map(lambda word: (word, 1))\n",
        "        | 'Group and sum' >> beam.CombinePerKey(sum)\n",
        "        | 'Top 5 words' >> beam.transforms.combiners.Top.Of(5, key=lambda x: x[1])\n",
        "        | 'Format results' >> beam.Map(lambda word_count: str(word_count))\n",
        "        | 'Write results' >> beam.io.WriteToText(outputs_prefix)\n",
        "    )"
      ],
      "metadata": {
        "id": "w96LNEC5Gfhc",
        "outputId": "66d4e483-940d-45e9-eef9-a6f714c3eb54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "class SplitRow(beam.DoFn):   #creating class inheriting DoFn class\n",
        "\n",
        "  def process(self, element):   #write the process function as per requirements\n",
        "    # return type -> list\n",
        "    return  [element.split(',')]\n",
        "\n",
        "\n",
        "p1 = beam.Pipeline()\n",
        "\n",
        "attendance_count = (\n",
        "\n",
        "   p1\n",
        "    |beam.io.ReadFromText('./dept_data.txt')\n",
        "\n",
        "    |beam.ParDo(SplitRow())   #call the class\n",
        "\n",
        "    |beam.io.WriteToText('data/output_new_final')\n",
        "\n",
        ")\n",
        "\n",
        "p1.run()\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/output_new_final-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "JMiiz3lUsgQP",
        "outputId": "7b071356-1c1c-4704-e17a-af6323aa6f34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
              "          var jqueryScript = document.createElement('script');\n",
              "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
              "          jqueryScript.type = 'text/javascript';\n",
              "          jqueryScript.onload = function() {\n",
              "            var datatableScript = document.createElement('script');\n",
              "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
              "            datatableScript.type = 'text/javascript';\n",
              "            datatableScript.onload = function() {\n",
              "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
              "              window.interactive_beam_jquery(document).ready(function($){\n",
              "                \n",
              "              });\n",
              "            }\n",
              "            document.head.appendChild(datatableScript);\n",
              "          };\n",
              "          document.head.appendChild(jqueryScript);\n",
              "        } else {\n",
              "          window.interactive_beam_jquery(document).ready(function($){\n",
              "            \n",
              "          });\n",
              "        }"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['149633CM', 'Marco', '10', 'Accounts', '1-01-2019']\n",
            "['212539MU', 'Rebekah', '10', 'Accounts', '1-01-2019']\n",
            "['231555ZZ', 'Itoe', '10', 'Accounts', '1-01-2019']\n",
            "['503996WI', 'Edouard', '10', 'Accounts', '1-01-2019']\n",
            "['704275DC', 'Kyle', '10', 'Accounts', '1-01-2019']\n",
            "['957149WC', 'Kyle', '10', 'Accounts', '1-01-2019']\n",
            "['241316NX', 'Kumiko', '10', 'Accounts', '1-01-2019']\n",
            "['796656IE', 'Gaston', '10', 'Accounts', '1-01-2019']\n",
            "['331593PS', 'Beryl', '20', 'HR', '1-01-2019']\n",
            "['560447WH', 'Olga', '20', 'HR', '1-01-2019']\n",
            "['222997TJ', 'Leslie', '20', 'HR', '1-01-2019']\n",
            "['171752SY', 'Mindy', '20', 'HR', '1-01-2019']\n",
            "['153636AS', 'Vicky', '20', 'HR', '1-01-2019']\n",
            "['745411HT', 'Richard', '20', 'HR', '1-01-2019']\n",
            "['298464HN', 'Kirk', '20', 'HR', '1-01-2019']\n",
            "['783950BW', 'Kaori', '20', 'HR', '1-01-2019']\n",
            "['892691AR', 'Beryl', '20', 'HR', '1-01-2019']\n",
            "['245668UZ', 'Oscar', '20', 'HR', '1-01-2019']\n",
            "['231206QD', 'Kumiko', '30', 'Finance', '1-01-2019']\n",
            "['357919KT', 'Wendy', '30', 'Finance', '1-01-2019']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "class SplitRow(beam.DoFn):   #creating class inheriting DoFn class\n",
        "\n",
        "  def process(self, element):   #write the process function as per requirements\n",
        "    # return type -> list\n",
        "    return  element.split(',')\n",
        "\n",
        "\n",
        "p1 = beam.Pipeline()\n",
        "\n",
        "attendance_count = (\n",
        "\n",
        "   p1\n",
        "    |beam.io.ReadFromText('./dept_data.txt')\n",
        "\n",
        "    |beam.ParDo(SplitRow())   #call the class\n",
        "\n",
        "    |beam.io.WriteToText('data/output_new_final')\n",
        "\n",
        ")\n",
        "\n",
        "p1.run()\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/output_new_final-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpkeE8nHu1zM",
        "outputId": "8046bd54-ba4f-4a10-a0b1-2f1ec723b8ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "149633CM\n",
            "Marco\n",
            "10\n",
            "Accounts\n",
            "1-01-2019\n",
            "212539MU\n",
            "Rebekah\n",
            "10\n",
            "Accounts\n",
            "1-01-2019\n",
            "231555ZZ\n",
            "Itoe\n",
            "10\n",
            "Accounts\n",
            "1-01-2019\n",
            "503996WI\n",
            "Edouard\n",
            "10\n",
            "Accounts\n",
            "1-01-2019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "class SplitRow(beam.DoFn):\n",
        "\n",
        "  def process(self, element):\n",
        "    # return type -> list\n",
        "    return  [element.split(',')]\n",
        "\n",
        "\n",
        "class FilterAccountsEmployee(beam.DoFn):\n",
        "\n",
        "  def process(self, element):\n",
        "    if element[3] == 'Accounts':\n",
        "      return [element]\n",
        "\n",
        "p1 = beam.Pipeline()\n",
        "\n",
        "attendance_count = (\n",
        "\n",
        "   p1\n",
        "    |beam.io.ReadFromText('dept_data.txt')\n",
        "\n",
        "    |beam.ParDo(SplitRow())\n",
        "   # | 'Compute WordLength' >> beam.ParDo(lambda element: [ element.split(',') ])\n",
        "\n",
        "    |beam.ParDo(FilterAccountsEmployee())\n",
        "\n",
        "    |beam.io.WriteToText('data/output_new_final')\n",
        "\n",
        ")\n",
        "\n",
        "p1.run()\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/output_new_final-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNHGOvtKvNA4",
        "outputId": "76b27174-dcda-4d66-bdae-453099cf2c86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['149633CM', 'Marco', '10', 'Accounts', '1-01-2019']\n",
            "['212539MU', 'Rebekah', '10', 'Accounts', '1-01-2019']\n",
            "['231555ZZ', 'Itoe', '10', 'Accounts', '1-01-2019']\n",
            "['503996WI', 'Edouard', '10', 'Accounts', '1-01-2019']\n",
            "['704275DC', 'Kyle', '10', 'Accounts', '1-01-2019']\n",
            "['957149WC', 'Kyle', '10', 'Accounts', '1-01-2019']\n",
            "['241316NX', 'Kumiko', '10', 'Accounts', '1-01-2019']\n",
            "['796656IE', 'Gaston', '10', 'Accounts', '1-01-2019']\n",
            "['149633CM', 'Marco', '10', 'Accounts', '2-01-2019']\n",
            "['212539MU', 'Rebekah', '10', 'Accounts', '2-01-2019']\n",
            "['231555ZZ', 'Itoe', '10', 'Accounts', '2-01-2019']\n",
            "['503996WI', 'Edouard', '10', 'Accounts', '2-01-2019']\n",
            "['704275DC', 'Kyle', '10', 'Accounts', '2-01-2019']\n",
            "['957149WC', 'Kyle', '10', 'Accounts', '2-01-2019']\n",
            "['241316NX', 'Kumiko', '10', 'Accounts', '2-01-2019']\n",
            "['796656IE', 'Gaston', '10', 'Accounts', '2-01-2019']\n",
            "['718737IX', 'Ayumi', '10', 'Accounts', '2-01-2019']\n",
            "['149633CM', 'Marco', '10', 'Accounts', '3-01-2019']\n",
            "['212539MU', 'Rebekah', '10', 'Accounts', '3-01-2019']\n",
            "['231555ZZ', 'Itoe', '10', 'Accounts', '3-01-2019']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "class SplitRow(beam.DoFn):\n",
        "\n",
        "  def process(self, element):\n",
        "    # return type -> list\n",
        "    return  [element.split(',')]\n",
        "\n",
        "\n",
        "class FilterAccountsEmployee(beam.DoFn):\n",
        "\n",
        "  def process(self, element):\n",
        "    if element[3] == 'Accounts':\n",
        "      return [element]\n",
        "\n",
        "class PairEmployees(beam.DoFn):\n",
        "\n",
        "  def process(self, element):\n",
        "    return [(element[3]+\",\"+element[1], 1)]\n",
        "\n",
        "class Counting(beam.DoFn):\n",
        "\n",
        "  def process(self, element):\n",
        "    # return type -> list\n",
        "    (key, values) = element           # [Marco, Accounts  [1,1,1,1....] , Rebekah, Accounts [1,1,1,1,....] ]\n",
        "    return [(key, sum(values))]\n",
        "\n",
        "\n",
        "p1 = beam.Pipeline()\n",
        "\n",
        "attendance_count = (\n",
        "\n",
        "   p1\n",
        "    |beam.io.ReadFromText('./dept_data.txt')\n",
        "\n",
        "    |beam.ParDo(SplitRow())\n",
        "   # | 'Compute WordLength' >> beam.ParDo(lambda element: [ element.split(',') ])\n",
        "\n",
        "    |beam.ParDo(FilterAccountsEmployee())\n",
        "    |beam.ParDo(PairEmployees())\n",
        "    | 'Group ' >> beam.GroupByKey()\n",
        "    | 'Sum using ParDo' >> beam.ParDo(Counting())\n",
        "\n",
        "    |beam.io.WriteToText('data/output_new_final')\n",
        "\n",
        ")\n",
        "\n",
        "p1.run()\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/output_new_final-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XuuMXl_v-UL",
        "outputId": "675c497d-2242-426d-924e-31be1503a013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Accounts,Marco', 31)\n",
            "('Accounts,Rebekah', 31)\n",
            "('Accounts,Itoe', 31)\n",
            "('Accounts,Edouard', 31)\n",
            "('Accounts,Kyle', 62)\n",
            "('Accounts,Kumiko', 31)\n",
            "('Accounts,Gaston', 31)\n",
            "('Accounts,Ayumi', 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "p = beam.Pipeline()\n",
        "\n",
        "class AverageFn(beam.CombineFn):\n",
        "\n",
        "  def create_accumulator(self):\n",
        "     return (0.0, 0)   # initialize (sum, count)\n",
        "\n",
        "  def add_input(self, sum_count, input):\n",
        "    (sum, count) = sum_count\n",
        "    return sum + input, count + 1\n",
        "\n",
        "  def merge_accumulators(self, accumulators):\n",
        "\n",
        "    ind_sums, ind_counts = zip(*accumulators)       # zip - [(27, 3), (39, 3), (18, 2)]  -->   [(27,39,18), (3,3,2)]\n",
        "    return sum(ind_sums), sum(ind_counts)        # (84,8)\n",
        "\n",
        "  def extract_output(self, sum_count):\n",
        "\n",
        "    (sum, count) = sum_count    # combine globally using CombineFn\n",
        "    return sum / count if count else float('NaN')\n",
        "\n",
        "\n",
        "small_sum = (\n",
        "           p\n",
        "            | beam.Create([15,5,7,7,9,23,13,5])\n",
        "            | \"Combine Globally\" >> beam.CombineGlobally(AverageFn())\n",
        "            | 'Write results' >> beam.io.WriteToText('data/combine')\n",
        "          )\n",
        "p.run()\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{'head -n 20 data/combine-00000-of-00001'}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "OFGi3euGlxX2",
        "outputId": "4a651db0-8ee4-4357-acc7-53a0feb1403f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
              "          var jqueryScript = document.createElement('script');\n",
              "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
              "          jqueryScript.type = 'text/javascript';\n",
              "          jqueryScript.onload = function() {\n",
              "            var datatableScript = document.createElement('script');\n",
              "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
              "            datatableScript.type = 'text/javascript';\n",
              "            datatableScript.onload = function() {\n",
              "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
              "              window.interactive_beam_jquery(document).ready(function($){\n",
              "                \n",
              "              });\n",
              "            }\n",
              "            document.head.appendChild(datatableScript);\n",
              "          };\n",
              "          document.head.appendChild(jqueryScript);\n",
              "        } else {\n",
              "          window.interactive_beam_jquery(document).ready(function($){\n",
              "            \n",
              "          });\n",
              "        }"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "class MyTransform(beam.PTransform):\n",
        "\n",
        "  def expand(self, input_coll):\n",
        "\n",
        "    a = (\n",
        "        input_coll\n",
        "                       | 'Group and sum1' >> beam.CombinePerKey(sum)\n",
        "                       | 'count filter accounts' >> beam.Filter(filter_on_count)\n",
        "                       | 'Regular accounts employee' >> beam.Map(format_output)\n",
        "\n",
        "    )\n",
        "    return a\n",
        "\n",
        "def SplitRow(element):\n",
        "    return element.split(',')\n",
        "\n",
        "\n",
        "def filter_on_count(element):\n",
        "  name, count = element\n",
        "  if count > 30:\n",
        "    return element\n",
        "\n",
        "def format_output(element):\n",
        "  name, count = element\n",
        "  #return ', '.join((name.encode('ascii'),str(count),'Regular employee'))\n",
        "  return ', '.join((name,str(count),'Regular employee'))\n",
        "\n",
        "p = beam.Pipeline()\n",
        "\n",
        "input_collection = (\n",
        "                      p\n",
        "                      | \"Read from text file\" >> beam.io.ReadFromText('./dept_data.txt')\n",
        "                      | \"Split rows\" >> beam.Map(SplitRow)\n",
        "                   )\n",
        "\n",
        "accounts_count = (\n",
        "                      input_collection\n",
        "                      | 'Get all Accounts dept persons' >> beam.Filter(lambda record: record[3] == 'Accounts')\n",
        "                      | 'Pair each accounts employee with 1' >> beam.Map(lambda record: (\"Accounts, \" +record[1], 1))\n",
        "                      | 'composite accoubts' >> MyTransform()\n",
        "                      | 'Write results for account' >> beam.io.WriteToText('data/Account')\n",
        "                 )\n",
        "\n",
        "hr_count = (\n",
        "                input_collection\n",
        "                | 'Get all HR dept persons' >> beam.Filter(lambda record: record[3] == 'HR')\n",
        "                | 'Pair each hr employee with 1' >> beam.Map(lambda record: (\"HR, \" +record[1], 1))\n",
        "                | 'composite HR' >> MyTransform()\n",
        "                | 'Write results for hr' >> beam.io.WriteToText('data/HR')\n",
        "           )\n",
        "p.run()\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/Account-00000-of-00001')}\n",
        "!{('head -n 20 data/HR-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URqLOCPHuHIl",
        "outputId": "71383e4c-eaae-4835-cb4e-6caeb06137b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accounts, Marco, 31, Regular employee\n",
            "Accounts, Rebekah, 31, Regular employee\n",
            "Accounts, Itoe, 31, Regular employee\n",
            "Accounts, Edouard, 31, Regular employee\n",
            "Accounts, Kyle, 62, Regular employee\n",
            "Accounts, Kumiko, 31, Regular employee\n",
            "Accounts, Gaston, 31, Regular employee\n",
            "HR, Beryl, 62, Regular employee\n",
            "HR, Olga, 31, Regular employee\n",
            "HR, Leslie, 31, Regular employee\n",
            "HR, Mindy, 31, Regular employee\n",
            "HR, Vicky, 31, Regular employee\n",
            "HR, Richard, 31, Regular employee\n",
            "HR, Kirk, 31, Regular employee\n",
            "HR, Kaori, 31, Regular employee\n",
            "HR, Oscar, 31, Regular employee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "def retTuple(element):\n",
        "\n",
        "  thisTuple=element.split(',')\n",
        "  return (thisTuple[0],thisTuple[1:])\n",
        "\n",
        "p1 = beam.Pipeline()\n",
        "\n",
        "# Apply a ParDo to the PCollection \"words\" to compute lengths for each word.\n",
        "dep_rows = (\n",
        "                p1\n",
        "                | \"Reading File 1\" >> beam.io.ReadFromText('./dept_data.txt')\n",
        "                | 'Pair each employee with key' >> beam.Map(retTuple)          # {149633CM : [Marco,10,Accounts,1-01-2019]}\n",
        "\n",
        "               )\n",
        "\n",
        "\n",
        "loc_rows = (\n",
        "                p1\n",
        "                | \"Reading File 2\" >> beam.io.ReadFromText('./location.txt')\n",
        "                | 'Pair each loc with key' >> beam.Map(retTuple)                # {149633CM : [9876843261,New York]}\n",
        "               )\n",
        "\n",
        "\n",
        "results = ({'dep_data': dep_rows, 'loc_data': loc_rows}\n",
        "\n",
        "           | beam.CoGroupByKey()\n",
        "           | 'Write results' >> beam.io.WriteToText('data/result')\n",
        "          )\n",
        "\n",
        "\n",
        "p1.run()\n",
        "\n",
        "!{('head -n 20 data/result-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cc_NmnyMvh6U",
        "outputId": "5a4706cf-6da8-47ec-80d8-b6a6588a6e9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('149633CM', {'dep_data': [['Marco', '10', 'Accounts', '1-01-2019'], ['Marco', '10', 'Accounts', '2-01-2019'], ['Marco', '10', 'Accounts', '3-01-2019'], ['Marco', '10', 'Accounts', '4-01-2019'], ['Marco', '10', 'Accounts', '5-01-2019'], ['Marco', '10', 'Accounts', '6-01-2019'], ['Marco', '10', 'Accounts', '7-01-2019'], ['Marco', '10', 'Accounts', '8-01-2019'], ['Marco', '10', 'Accounts', '9-01-2019'], ['Marco', '10', 'Accounts', '10-01-2019'], ['Marco', '10', 'Accounts', '11-01-2019'], ['Marco', '10', 'Accounts', '12-01-2019'], ['Marco', '10', 'Accounts', '13-01-2019'], ['Marco', '10', 'Accounts', '14-01-2019'], ['Marco', '10', 'Accounts', '15-01-2019'], ['Marco', '10', 'Accounts', '16-01-2019'], ['Marco', '10', 'Accounts', '17-01-2019'], ['Marco', '10', 'Accounts', '18-01-2019'], ['Marco', '10', 'Accounts', '19-01-2019'], ['Marco', '10', 'Accounts', '20-01-2019'], ['Marco', '10', 'Accounts', '21-01-2019'], ['Marco', '10', 'Accounts', '22-01-2019'], ['Marco', '10', 'Accounts', '23-01-2019'], ['Marco', '10', 'Accounts', '24-01-2019'], ['Marco', '10', 'Accounts', '25-01-2019'], ['Marco', '10', 'Accounts', '26-01-2019'], ['Marco', '10', 'Accounts', '27-01-2019'], ['Marco', '10', 'Accounts', '28-01-2019'], ['Marco', '10', 'Accounts', '29-01-2019'], ['Marco', '10', 'Accounts', '30-01-2019'], ['Marco', '10', 'Accounts', '31-01-2019']], 'loc_data': [['9876843261', 'New York'], ['9204232778', 'New York']]})\n",
            "('212539MU', {'dep_data': [['Rebekah', '10', 'Accounts', '1-01-2019'], ['Rebekah', '10', 'Accounts', '2-01-2019'], ['Rebekah', '10', 'Accounts', '3-01-2019'], ['Rebekah', '10', 'Accounts', '4-01-2019'], ['Rebekah', '10', 'Accounts', '5-01-2019'], ['Rebekah', '10', 'Accounts', '6-01-2019'], ['Rebekah', '10', 'Accounts', '7-01-2019'], ['Rebekah', '10', 'Accounts', '8-01-2019'], ['Rebekah', '10', 'Accounts', '9-01-2019'], ['Rebekah', '10', 'Accounts', '10-01-2019'], ['Rebekah', '10', 'Accounts', '11-01-2019'], ['Rebekah', '10', 'Accounts', '12-01-2019'], ['Rebekah', '10', 'Accounts', '13-01-2019'], ['Rebekah', '10', 'Accounts', '14-01-2019'], ['Rebekah', '10', 'Accounts', '15-01-2019'], ['Rebekah', '10', 'Accounts', '16-01-2019'], ['Rebekah', '10', 'Accounts', '17-01-2019'], ['Rebekah', '10', 'Accounts', '18-01-2019'], ['Rebekah', '10', 'Accounts', '19-01-2019'], ['Rebekah', '10', 'Accounts', '20-01-2019'], ['Rebekah', '10', 'Accounts', '21-01-2019'], ['Rebekah', '10', 'Accounts', '22-01-2019'], ['Rebekah', '10', 'Accounts', '23-01-2019'], ['Rebekah', '10', 'Accounts', '24-01-2019'], ['Rebekah', '10', 'Accounts', '25-01-2019'], ['Rebekah', '10', 'Accounts', '26-01-2019'], ['Rebekah', '10', 'Accounts', '27-01-2019'], ['Rebekah', '10', 'Accounts', '28-01-2019'], ['Rebekah', '10', 'Accounts', '29-01-2019'], ['Rebekah', '10', 'Accounts', '30-01-2019'], ['Rebekah', '10', 'Accounts', '31-01-2019']], 'loc_data': [['9995440673', 'Denver']]})\n",
            "('231555ZZ', {'dep_data': [['Itoe', '10', 'Accounts', '1-01-2019'], ['Itoe', '10', 'Accounts', '2-01-2019'], ['Itoe', '10', 'Accounts', '3-01-2019'], ['Itoe', '10', 'Accounts', '4-01-2019'], ['Itoe', '10', 'Accounts', '5-01-2019'], ['Itoe', '10', 'Accounts', '6-01-2019'], ['Itoe', '10', 'Accounts', '7-01-2019'], ['Itoe', '10', 'Accounts', '8-01-2019'], ['Itoe', '10', 'Accounts', '9-01-2019'], ['Itoe', '10', 'Accounts', '10-01-2019'], ['Itoe', '10', 'Accounts', '11-01-2019'], ['Itoe', '10', 'Accounts', '12-01-2019'], ['Itoe', '10', 'Accounts', '13-01-2019'], ['Itoe', '10', 'Accounts', '14-01-2019'], ['Itoe', '10', 'Accounts', '15-01-2019'], ['Itoe', '10', 'Accounts', '16-01-2019'], ['Itoe', '10', 'Accounts', '17-01-2019'], ['Itoe', '10', 'Accounts', '18-01-2019'], ['Itoe', '10', 'Accounts', '19-01-2019'], ['Itoe', '10', 'Accounts', '20-01-2019'], ['Itoe', '10', 'Accounts', '21-01-2019'], ['Itoe', '10', 'Accounts', '22-01-2019'], ['Itoe', '10', 'Accounts', '23-01-2019'], ['Itoe', '10', 'Accounts', '24-01-2019'], ['Itoe', '10', 'Accounts', '25-01-2019'], ['Itoe', '10', 'Accounts', '26-01-2019'], ['Itoe', '10', 'Accounts', '27-01-2019'], ['Itoe', '10', 'Accounts', '28-01-2019'], ['Itoe', '10', 'Accounts', '29-01-2019'], ['Itoe', '10', 'Accounts', '30-01-2019'], ['Itoe', '10', 'Accounts', '31-01-2019']], 'loc_data': [['9196597290', 'Boston']]})\n",
            "('503996WI', {'dep_data': [['Edouard', '10', 'Accounts', '1-01-2019'], ['Edouard', '10', 'Accounts', '2-01-2019'], ['Edouard', '10', 'Accounts', '3-01-2019'], ['Edouard', '10', 'Accounts', '4-01-2019'], ['Edouard', '10', 'Accounts', '5-01-2019'], ['Edouard', '10', 'Accounts', '6-01-2019'], ['Edouard', '10', 'Accounts', '7-01-2019'], ['Edouard', '10', 'Accounts', '8-01-2019'], ['Edouard', '10', 'Accounts', '9-01-2019'], ['Edouard', '10', 'Accounts', '10-01-2019'], ['Edouard', '10', 'Accounts', '11-01-2019'], ['Edouard', '10', 'Accounts', '12-01-2019'], ['Edouard', '10', 'Accounts', '13-01-2019'], ['Edouard', '10', 'Accounts', '14-01-2019'], ['Edouard', '10', 'Accounts', '15-01-2019'], ['Edouard', '10', 'Accounts', '16-01-2019'], ['Edouard', '10', 'Accounts', '17-01-2019'], ['Edouard', '10', 'Accounts', '18-01-2019'], ['Edouard', '10', 'Accounts', '19-01-2019'], ['Edouard', '10', 'Accounts', '20-01-2019'], ['Edouard', '10', 'Accounts', '21-01-2019'], ['Edouard', '10', 'Accounts', '22-01-2019'], ['Edouard', '10', 'Accounts', '23-01-2019'], ['Edouard', '10', 'Accounts', '24-01-2019'], ['Edouard', '10', 'Accounts', '25-01-2019'], ['Edouard', '10', 'Accounts', '26-01-2019'], ['Edouard', '10', 'Accounts', '27-01-2019'], ['Edouard', '10', 'Accounts', '28-01-2019'], ['Edouard', '10', 'Accounts', '29-01-2019'], ['Edouard', '10', 'Accounts', '30-01-2019'], ['Edouard', '10', 'Accounts', '31-01-2019']], 'loc_data': [['9468234252', 'Miami']]})\n",
            "('704275DC', {'dep_data': [['Kyle', '10', 'Accounts', '1-01-2019'], ['Kyle', '10', 'Accounts', '2-01-2019'], ['Kyle', '10', 'Accounts', '3-01-2019'], ['Kyle', '10', 'Accounts', '4-01-2019'], ['Kyle', '10', 'Accounts', '5-01-2019'], ['Kyle', '10', 'Accounts', '6-01-2019'], ['Kyle', '10', 'Accounts', '7-01-2019'], ['Kyle', '10', 'Accounts', '8-01-2019'], ['Kyle', '10', 'Accounts', '9-01-2019'], ['Kyle', '10', 'Accounts', '10-01-2019'], ['Kyle', '10', 'Accounts', '11-01-2019'], ['Kyle', '10', 'Accounts', '12-01-2019'], ['Kyle', '10', 'Accounts', '13-01-2019'], ['Kyle', '10', 'Accounts', '14-01-2019'], ['Kyle', '10', 'Accounts', '15-01-2019'], ['Kyle', '10', 'Accounts', '16-01-2019'], ['Kyle', '10', 'Accounts', '17-01-2019'], ['Kyle', '10', 'Accounts', '18-01-2019'], ['Kyle', '10', 'Accounts', '19-01-2019'], ['Kyle', '10', 'Accounts', '20-01-2019'], ['Kyle', '10', 'Accounts', '21-01-2019'], ['Kyle', '10', 'Accounts', '22-01-2019'], ['Kyle', '10', 'Accounts', '23-01-2019'], ['Kyle', '10', 'Accounts', '24-01-2019'], ['Kyle', '10', 'Accounts', '25-01-2019'], ['Kyle', '10', 'Accounts', '26-01-2019'], ['Kyle', '10', 'Accounts', '27-01-2019'], ['Kyle', '10', 'Accounts', '28-01-2019'], ['Kyle', '10', 'Accounts', '29-01-2019'], ['Kyle', '10', 'Accounts', '30-01-2019'], ['Kyle', '10', 'Accounts', '31-01-2019']], 'loc_data': [['9776235961', 'Miami']]})\n",
            "('957149WC', {'dep_data': [['Kyle', '10', 'Accounts', '1-01-2019'], ['Kyle', '10', 'Accounts', '2-01-2019'], ['Kyle', '10', 'Accounts', '3-01-2019'], ['Kyle', '10', 'Accounts', '4-01-2019'], ['Kyle', '10', 'Accounts', '5-01-2019'], ['Kyle', '10', 'Accounts', '6-01-2019'], ['Kyle', '10', 'Accounts', '7-01-2019'], ['Kyle', '10', 'Accounts', '8-01-2019'], ['Kyle', '10', 'Accounts', '9-01-2019'], ['Kyle', '10', 'Accounts', '10-01-2019'], ['Kyle', '10', 'Accounts', '11-01-2019'], ['Kyle', '10', 'Accounts', '12-01-2019'], ['Kyle', '10', 'Accounts', '13-01-2019'], ['Kyle', '10', 'Accounts', '14-01-2019'], ['Kyle', '10', 'Accounts', '15-01-2019'], ['Kyle', '10', 'Accounts', '16-01-2019'], ['Kyle', '10', 'Accounts', '17-01-2019'], ['Kyle', '10', 'Accounts', '18-01-2019'], ['Kyle', '10', 'Accounts', '19-01-2019'], ['Kyle', '10', 'Accounts', '20-01-2019'], ['Kyle', '10', 'Accounts', '21-01-2019'], ['Kyle', '10', 'Accounts', '22-01-2019'], ['Kyle', '10', 'Accounts', '23-01-2019'], ['Kyle', '10', 'Accounts', '24-01-2019'], ['Kyle', '10', 'Accounts', '25-01-2019'], ['Kyle', '10', 'Accounts', '26-01-2019'], ['Kyle', '10', 'Accounts', '27-01-2019'], ['Kyle', '10', 'Accounts', '28-01-2019'], ['Kyle', '10', 'Accounts', '29-01-2019'], ['Kyle', '10', 'Accounts', '30-01-2019'], ['Kyle', '10', 'Accounts', '31-01-2019']], 'loc_data': [['9925595092', 'Houston']]})\n",
            "('241316NX', {'dep_data': [['Kumiko', '10', 'Accounts', '1-01-2019'], ['Kumiko', '10', 'Accounts', '2-01-2019'], ['Kumiko', '10', 'Accounts', '3-01-2019'], ['Kumiko', '10', 'Accounts', '4-01-2019'], ['Kumiko', '10', 'Accounts', '5-01-2019'], ['Kumiko', '10', 'Accounts', '6-01-2019'], ['Kumiko', '10', 'Accounts', '7-01-2019'], ['Kumiko', '10', 'Accounts', '8-01-2019'], ['Kumiko', '10', 'Accounts', '9-01-2019'], ['Kumiko', '10', 'Accounts', '10-01-2019'], ['Kumiko', '10', 'Accounts', '11-01-2019'], ['Kumiko', '10', 'Accounts', '12-01-2019'], ['Kumiko', '10', 'Accounts', '13-01-2019'], ['Kumiko', '10', 'Accounts', '14-01-2019'], ['Kumiko', '10', 'Accounts', '15-01-2019'], ['Kumiko', '10', 'Accounts', '16-01-2019'], ['Kumiko', '10', 'Accounts', '17-01-2019'], ['Kumiko', '10', 'Accounts', '18-01-2019'], ['Kumiko', '10', 'Accounts', '19-01-2019'], ['Kumiko', '10', 'Accounts', '20-01-2019'], ['Kumiko', '10', 'Accounts', '21-01-2019'], ['Kumiko', '10', 'Accounts', '22-01-2019'], ['Kumiko', '10', 'Accounts', '23-01-2019'], ['Kumiko', '10', 'Accounts', '24-01-2019'], ['Kumiko', '10', 'Accounts', '25-01-2019'], ['Kumiko', '10', 'Accounts', '26-01-2019'], ['Kumiko', '10', 'Accounts', '27-01-2019'], ['Kumiko', '10', 'Accounts', '28-01-2019'], ['Kumiko', '10', 'Accounts', '29-01-2019'], ['Kumiko', '10', 'Accounts', '30-01-2019'], ['Kumiko', '10', 'Accounts', '31-01-2019']], 'loc_data': [['9837402343', 'Boston']]})\n",
            "('796656IE', {'dep_data': [['Gaston', '10', 'Accounts', '1-01-2019'], ['Gaston', '10', 'Accounts', '2-01-2019'], ['Gaston', '10', 'Accounts', '3-01-2019'], ['Gaston', '10', 'Accounts', '4-01-2019'], ['Gaston', '10', 'Accounts', '5-01-2019'], ['Gaston', '10', 'Accounts', '6-01-2019'], ['Gaston', '10', 'Accounts', '7-01-2019'], ['Gaston', '10', 'Accounts', '8-01-2019'], ['Gaston', '10', 'Accounts', '9-01-2019'], ['Gaston', '10', 'Accounts', '10-01-2019'], ['Gaston', '10', 'Accounts', '11-01-2019'], ['Gaston', '10', 'Accounts', '12-01-2019'], ['Gaston', '10', 'Accounts', '13-01-2019'], ['Gaston', '10', 'Accounts', '14-01-2019'], ['Gaston', '10', 'Accounts', '15-01-2019'], ['Gaston', '10', 'Accounts', '16-01-2019'], ['Gaston', '10', 'Accounts', '17-01-2019'], ['Gaston', '10', 'Accounts', '18-01-2019'], ['Gaston', '10', 'Accounts', '19-01-2019'], ['Gaston', '10', 'Accounts', '20-01-2019'], ['Gaston', '10', 'Accounts', '21-01-2019'], ['Gaston', '10', 'Accounts', '22-01-2019'], ['Gaston', '10', 'Accounts', '23-01-2019'], ['Gaston', '10', 'Accounts', '24-01-2019'], ['Gaston', '10', 'Accounts', '25-01-2019'], ['Gaston', '10', 'Accounts', '26-01-2019'], ['Gaston', '10', 'Accounts', '27-01-2019'], ['Gaston', '10', 'Accounts', '28-01-2019'], ['Gaston', '10', 'Accounts', '29-01-2019'], ['Gaston', '10', 'Accounts', '30-01-2019'], ['Gaston', '10', 'Accounts', '31-01-2019']], 'loc_data': [['9538848876', 'Houston']]})\n",
            "('331593PS', {'dep_data': [['Beryl', '20', 'HR', '1-01-2019'], ['Beryl', '20', 'HR', '2-01-2019'], ['Beryl', '20', 'HR', '3-01-2019'], ['Beryl', '20', 'HR', '4-01-2019'], ['Beryl', '20', 'HR', '5-01-2019'], ['Beryl', '20', 'HR', '6-01-2019'], ['Beryl', '20', 'HR', '7-01-2019'], ['Beryl', '20', 'HR', '8-01-2019'], ['Beryl', '20', 'HR', '9-01-2019'], ['Beryl', '20', 'HR', '10-01-2019'], ['Beryl', '20', 'HR', '11-01-2019'], ['Beryl', '20', 'HR', '12-01-2019'], ['Beryl', '20', 'HR', '13-01-2019'], ['Beryl', '20', 'HR', '14-01-2019'], ['Beryl', '20', 'HR', '15-01-2019'], ['Beryl', '20', 'HR', '16-01-2019'], ['Beryl', '20', 'HR', '17-01-2019'], ['Beryl', '20', 'HR', '18-01-2019'], ['Beryl', '20', 'HR', '19-01-2019'], ['Beryl', '20', 'HR', '20-01-2019'], ['Beryl', '20', 'HR', '21-01-2019'], ['Beryl', '20', 'HR', '22-01-2019'], ['Beryl', '20', 'HR', '23-01-2019'], ['Beryl', '20', 'HR', '24-01-2019'], ['Beryl', '20', 'HR', '25-01-2019'], ['Beryl', '20', 'HR', '26-01-2019'], ['Beryl', '20', 'HR', '27-01-2019'], ['Beryl', '20', 'HR', '28-01-2019'], ['Beryl', '20', 'HR', '29-01-2019'], ['Beryl', '20', 'HR', '30-01-2019'], ['Beryl', '20', 'HR', '31-01-2019']], 'loc_data': [['9137216186', 'Miami']]})\n",
            "('560447WH', {'dep_data': [['Olga', '20', 'HR', '1-01-2019'], ['Olga', '20', 'HR', '2-01-2019'], ['Olga', '20', 'HR', '3-01-2019'], ['Olga', '20', 'HR', '4-01-2019'], ['Olga', '20', 'HR', '5-01-2019'], ['Olga', '20', 'HR', '6-01-2019'], ['Olga', '20', 'HR', '7-01-2019'], ['Olga', '20', 'HR', '8-01-2019'], ['Olga', '20', 'HR', '9-01-2019'], ['Olga', '20', 'HR', '10-01-2019'], ['Olga', '20', 'HR', '11-01-2019'], ['Olga', '20', 'HR', '12-01-2019'], ['Olga', '20', 'HR', '13-01-2019'], ['Olga', '20', 'HR', '14-01-2019'], ['Olga', '20', 'HR', '15-01-2019'], ['Olga', '20', 'HR', '16-01-2019'], ['Olga', '20', 'HR', '17-01-2019'], ['Olga', '20', 'HR', '18-01-2019'], ['Olga', '20', 'HR', '19-01-2019'], ['Olga', '20', 'HR', '20-01-2019'], ['Olga', '20', 'HR', '21-01-2019'], ['Olga', '20', 'HR', '22-01-2019'], ['Olga', '20', 'HR', '23-01-2019'], ['Olga', '20', 'HR', '24-01-2019'], ['Olga', '20', 'HR', '25-01-2019'], ['Olga', '20', 'HR', '26-01-2019'], ['Olga', '20', 'HR', '27-01-2019'], ['Olga', '20', 'HR', '28-01-2019'], ['Olga', '20', 'HR', '29-01-2019'], ['Olga', '20', 'HR', '30-01-2019'], ['Olga', '20', 'HR', '31-01-2019']], 'loc_data': [['9708010864', 'Miami']]})\n",
            "('222997TJ', {'dep_data': [['Leslie', '20', 'HR', '1-01-2019'], ['Leslie', '20', 'HR', '2-01-2019'], ['Leslie', '20', 'HR', '3-01-2019'], ['Leslie', '20', 'HR', '4-01-2019'], ['Leslie', '20', 'HR', '5-01-2019'], ['Leslie', '20', 'HR', '6-01-2019'], ['Leslie', '20', 'HR', '7-01-2019'], ['Leslie', '20', 'HR', '8-01-2019'], ['Leslie', '20', 'HR', '9-01-2019'], ['Leslie', '20', 'HR', '10-01-2019'], ['Leslie', '20', 'HR', '11-01-2019'], ['Leslie', '20', 'HR', '12-01-2019'], ['Leslie', '20', 'HR', '13-01-2019'], ['Leslie', '20', 'HR', '14-01-2019'], ['Leslie', '20', 'HR', '15-01-2019'], ['Leslie', '20', 'HR', '16-01-2019'], ['Leslie', '20', 'HR', '17-01-2019'], ['Leslie', '20', 'HR', '18-01-2019'], ['Leslie', '20', 'HR', '19-01-2019'], ['Leslie', '20', 'HR', '20-01-2019'], ['Leslie', '20', 'HR', '21-01-2019'], ['Leslie', '20', 'HR', '22-01-2019'], ['Leslie', '20', 'HR', '23-01-2019'], ['Leslie', '20', 'HR', '24-01-2019'], ['Leslie', '20', 'HR', '25-01-2019'], ['Leslie', '20', 'HR', '26-01-2019'], ['Leslie', '20', 'HR', '27-01-2019'], ['Leslie', '20', 'HR', '28-01-2019'], ['Leslie', '20', 'HR', '29-01-2019'], ['Leslie', '20', 'HR', '30-01-2019'], ['Leslie', '20', 'HR', '31-01-2019']], 'loc_data': [['9410006713', 'Washington']]})\n",
            "('171752SY', {'dep_data': [['Mindy', '20', 'HR', '1-01-2019'], ['Mindy', '20', 'HR', '2-01-2019'], ['Mindy', '20', 'HR', '3-01-2019'], ['Mindy', '20', 'HR', '4-01-2019'], ['Mindy', '20', 'HR', '5-01-2019'], ['Mindy', '20', 'HR', '6-01-2019'], ['Mindy', '20', 'HR', '7-01-2019'], ['Mindy', '20', 'HR', '8-01-2019'], ['Mindy', '20', 'HR', '9-01-2019'], ['Mindy', '20', 'HR', '10-01-2019'], ['Mindy', '20', 'HR', '11-01-2019'], ['Mindy', '20', 'HR', '12-01-2019'], ['Mindy', '20', 'HR', '13-01-2019'], ['Mindy', '20', 'HR', '14-01-2019'], ['Mindy', '20', 'HR', '15-01-2019'], ['Mindy', '20', 'HR', '16-01-2019'], ['Mindy', '20', 'HR', '17-01-2019'], ['Mindy', '20', 'HR', '18-01-2019'], ['Mindy', '20', 'HR', '19-01-2019'], ['Mindy', '20', 'HR', '20-01-2019'], ['Mindy', '20', 'HR', '21-01-2019'], ['Mindy', '20', 'HR', '22-01-2019'], ['Mindy', '20', 'HR', '23-01-2019'], ['Mindy', '20', 'HR', '24-01-2019'], ['Mindy', '20', 'HR', '25-01-2019'], ['Mindy', '20', 'HR', '26-01-2019'], ['Mindy', '20', 'HR', '27-01-2019'], ['Mindy', '20', 'HR', '28-01-2019'], ['Mindy', '20', 'HR', '29-01-2019'], ['Mindy', '20', 'HR', '30-01-2019'], ['Mindy', '20', 'HR', '31-01-2019']], 'loc_data': [['9494683837', 'Boston']]})\n",
            "('153636AS', {'dep_data': [['Vicky', '20', 'HR', '1-01-2019'], ['Vicky', '20', 'HR', '2-01-2019'], ['Vicky', '20', 'HR', '3-01-2019'], ['Vicky', '20', 'HR', '4-01-2019'], ['Vicky', '20', 'HR', '5-01-2019'], ['Vicky', '20', 'HR', '6-01-2019'], ['Vicky', '20', 'HR', '7-01-2019'], ['Vicky', '20', 'HR', '8-01-2019'], ['Vicky', '20', 'HR', '9-01-2019'], ['Vicky', '20', 'HR', '10-01-2019'], ['Vicky', '20', 'HR', '11-01-2019'], ['Vicky', '20', 'HR', '12-01-2019'], ['Vicky', '20', 'HR', '13-01-2019'], ['Vicky', '20', 'HR', '14-01-2019'], ['Vicky', '20', 'HR', '15-01-2019'], ['Vicky', '20', 'HR', '16-01-2019'], ['Vicky', '20', 'HR', '17-01-2019'], ['Vicky', '20', 'HR', '18-01-2019'], ['Vicky', '20', 'HR', '19-01-2019'], ['Vicky', '20', 'HR', '20-01-2019'], ['Vicky', '20', 'HR', '21-01-2019'], ['Vicky', '20', 'HR', '22-01-2019'], ['Vicky', '20', 'HR', '23-01-2019'], ['Vicky', '20', 'HR', '24-01-2019'], ['Vicky', '20', 'HR', '25-01-2019'], ['Vicky', '20', 'HR', '26-01-2019'], ['Vicky', '20', 'HR', '27-01-2019'], ['Vicky', '20', 'HR', '28-01-2019'], ['Vicky', '20', 'HR', '29-01-2019'], ['Vicky', '20', 'HR', '30-01-2019'], ['Vicky', '20', 'HR', '31-01-2019']], 'loc_data': [['9575378417', 'New York']]})\n",
            "('745411HT', {'dep_data': [['Richard', '20', 'HR', '1-01-2019'], ['Richard', '20', 'HR', '2-01-2019'], ['Richard', '20', 'HR', '3-01-2019'], ['Richard', '20', 'HR', '4-01-2019'], ['Richard', '20', 'HR', '5-01-2019'], ['Richard', '20', 'HR', '6-01-2019'], ['Richard', '20', 'HR', '7-01-2019'], ['Richard', '20', 'HR', '8-01-2019'], ['Richard', '20', 'HR', '9-01-2019'], ['Richard', '20', 'HR', '10-01-2019'], ['Richard', '20', 'HR', '11-01-2019'], ['Richard', '20', 'HR', '12-01-2019'], ['Richard', '20', 'HR', '13-01-2019'], ['Richard', '20', 'HR', '14-01-2019'], ['Richard', '20', 'HR', '15-01-2019'], ['Richard', '20', 'HR', '16-01-2019'], ['Richard', '20', 'HR', '17-01-2019'], ['Richard', '20', 'HR', '18-01-2019'], ['Richard', '20', 'HR', '19-01-2019'], ['Richard', '20', 'HR', '20-01-2019'], ['Richard', '20', 'HR', '21-01-2019'], ['Richard', '20', 'HR', '22-01-2019'], ['Richard', '20', 'HR', '23-01-2019'], ['Richard', '20', 'HR', '24-01-2019'], ['Richard', '20', 'HR', '25-01-2019'], ['Richard', '20', 'HR', '26-01-2019'], ['Richard', '20', 'HR', '27-01-2019'], ['Richard', '20', 'HR', '28-01-2019'], ['Richard', '20', 'HR', '29-01-2019'], ['Richard', '20', 'HR', '30-01-2019'], ['Richard', '20', 'HR', '31-01-2019']], 'loc_data': [['9391632080', 'New York']]})\n",
            "('298464HN', {'dep_data': [['Kirk', '20', 'HR', '1-01-2019'], ['Kirk', '20', 'HR', '2-01-2019'], ['Kirk', '20', 'HR', '3-01-2019'], ['Kirk', '20', 'HR', '4-01-2019'], ['Kirk', '20', 'HR', '5-01-2019'], ['Kirk', '20', 'HR', '6-01-2019'], ['Kirk', '20', 'HR', '7-01-2019'], ['Kirk', '20', 'HR', '8-01-2019'], ['Kirk', '20', 'HR', '9-01-2019'], ['Kirk', '20', 'HR', '10-01-2019'], ['Kirk', '20', 'HR', '11-01-2019'], ['Kirk', '20', 'HR', '12-01-2019'], ['Kirk', '20', 'HR', '13-01-2019'], ['Kirk', '20', 'HR', '14-01-2019'], ['Kirk', '20', 'HR', '15-01-2019'], ['Kirk', '20', 'HR', '16-01-2019'], ['Kirk', '20', 'HR', '17-01-2019'], ['Kirk', '20', 'HR', '18-01-2019'], ['Kirk', '20', 'HR', '19-01-2019'], ['Kirk', '20', 'HR', '20-01-2019'], ['Kirk', '20', 'HR', '21-01-2019'], ['Kirk', '20', 'HR', '22-01-2019'], ['Kirk', '20', 'HR', '23-01-2019'], ['Kirk', '20', 'HR', '24-01-2019'], ['Kirk', '20', 'HR', '25-01-2019'], ['Kirk', '20', 'HR', '26-01-2019'], ['Kirk', '20', 'HR', '27-01-2019'], ['Kirk', '20', 'HR', '28-01-2019'], ['Kirk', '20', 'HR', '29-01-2019'], ['Kirk', '20', 'HR', '30-01-2019'], ['Kirk', '20', 'HR', '31-01-2019']], 'loc_data': [['9982641601', 'Miami']]})\n",
            "('783950BW', {'dep_data': [['Kaori', '20', 'HR', '1-01-2019'], ['Kaori', '20', 'HR', '2-01-2019'], ['Kaori', '20', 'HR', '3-01-2019'], ['Kaori', '20', 'HR', '4-01-2019'], ['Kaori', '20', 'HR', '5-01-2019'], ['Kaori', '20', 'HR', '6-01-2019'], ['Kaori', '20', 'HR', '7-01-2019'], ['Kaori', '20', 'HR', '8-01-2019'], ['Kaori', '20', 'HR', '9-01-2019'], ['Kaori', '20', 'HR', '10-01-2019'], ['Kaori', '20', 'HR', '11-01-2019'], ['Kaori', '20', 'HR', '12-01-2019'], ['Kaori', '20', 'HR', '13-01-2019'], ['Kaori', '20', 'HR', '14-01-2019'], ['Kaori', '20', 'HR', '15-01-2019'], ['Kaori', '20', 'HR', '16-01-2019'], ['Kaori', '20', 'HR', '17-01-2019'], ['Kaori', '20', 'HR', '18-01-2019'], ['Kaori', '20', 'HR', '19-01-2019'], ['Kaori', '20', 'HR', '20-01-2019'], ['Kaori', '20', 'HR', '21-01-2019'], ['Kaori', '20', 'HR', '22-01-2019'], ['Kaori', '20', 'HR', '23-01-2019'], ['Kaori', '20', 'HR', '24-01-2019'], ['Kaori', '20', 'HR', '25-01-2019'], ['Kaori', '20', 'HR', '26-01-2019'], ['Kaori', '20', 'HR', '27-01-2019'], ['Kaori', '20', 'HR', '28-01-2019'], ['Kaori', '20', 'HR', '29-01-2019'], ['Kaori', '20', 'HR', '30-01-2019'], ['Kaori', '20', 'HR', '31-01-2019']], 'loc_data': [['9248480224', 'Boston']]})\n",
            "('892691AR', {'dep_data': [['Beryl', '20', 'HR', '1-01-2019'], ['Beryl', '20', 'HR', '2-01-2019'], ['Beryl', '20', 'HR', '3-01-2019'], ['Beryl', '20', 'HR', '4-01-2019'], ['Beryl', '20', 'HR', '5-01-2019'], ['Beryl', '20', 'HR', '6-01-2019'], ['Beryl', '20', 'HR', '7-01-2019'], ['Beryl', '20', 'HR', '8-01-2019'], ['Beryl', '20', 'HR', '9-01-2019'], ['Beryl', '20', 'HR', '10-01-2019'], ['Beryl', '20', 'HR', '11-01-2019'], ['Beryl', '20', 'HR', '12-01-2019'], ['Beryl', '20', 'HR', '13-01-2019'], ['Beryl', '20', 'HR', '14-01-2019'], ['Beryl', '20', 'HR', '15-01-2019'], ['Beryl', '20', 'HR', '16-01-2019'], ['Beryl', '20', 'HR', '17-01-2019'], ['Beryl', '20', 'HR', '18-01-2019'], ['Beryl', '20', 'HR', '19-01-2019'], ['Beryl', '20', 'HR', '20-01-2019'], ['Beryl', '20', 'HR', '21-01-2019'], ['Beryl', '20', 'HR', '22-01-2019'], ['Beryl', '20', 'HR', '23-01-2019'], ['Beryl', '20', 'HR', '24-01-2019'], ['Beryl', '20', 'HR', '25-01-2019'], ['Beryl', '20', 'HR', '26-01-2019'], ['Beryl', '20', 'HR', '27-01-2019'], ['Beryl', '20', 'HR', '28-01-2019'], ['Beryl', '20', 'HR', '29-01-2019'], ['Beryl', '20', 'HR', '30-01-2019'], ['Beryl', '20', 'HR', '31-01-2019']], 'loc_data': [['9723803710', 'New York']]})\n",
            "('245668UZ', {'dep_data': [['Oscar', '20', 'HR', '1-01-2019'], ['Oscar', '20', 'HR', '2-01-2019'], ['Oscar', '20', 'HR', '3-01-2019'], ['Oscar', '20', 'HR', '4-01-2019'], ['Oscar', '20', 'HR', '5-01-2019'], ['Oscar', '20', 'HR', '6-01-2019'], ['Oscar', '20', 'HR', '7-01-2019'], ['Oscar', '20', 'HR', '8-01-2019'], ['Oscar', '20', 'HR', '9-01-2019'], ['Oscar', '20', 'HR', '10-01-2019'], ['Oscar', '20', 'HR', '11-01-2019'], ['Oscar', '20', 'HR', '12-01-2019'], ['Oscar', '20', 'HR', '13-01-2019'], ['Oscar', '20', 'HR', '14-01-2019'], ['Oscar', '20', 'HR', '15-01-2019'], ['Oscar', '20', 'HR', '16-01-2019'], ['Oscar', '20', 'HR', '17-01-2019'], ['Oscar', '20', 'HR', '18-01-2019'], ['Oscar', '20', 'HR', '19-01-2019'], ['Oscar', '20', 'HR', '20-01-2019'], ['Oscar', '20', 'HR', '21-01-2019'], ['Oscar', '20', 'HR', '22-01-2019'], ['Oscar', '20', 'HR', '23-01-2019'], ['Oscar', '20', 'HR', '24-01-2019'], ['Oscar', '20', 'HR', '25-01-2019'], ['Oscar', '20', 'HR', '26-01-2019'], ['Oscar', '20', 'HR', '27-01-2019'], ['Oscar', '20', 'HR', '28-01-2019'], ['Oscar', '20', 'HR', '29-01-2019'], ['Oscar', '20', 'HR', '30-01-2019'], ['Oscar', '20', 'HR', '31-01-2019']], 'loc_data': [['9395037841', 'New York']]})\n",
            "('231206QD', {'dep_data': [['Kumiko', '30', 'Finance', '1-01-2019'], ['Kumiko', '30', 'Finance', '2-01-2019'], ['Kumiko', '30', 'Finance', '3-01-2019'], ['Kumiko', '30', 'Finance', '4-01-2019'], ['Kumiko', '30', 'Finance', '5-01-2019'], ['Kumiko', '30', 'Finance', '6-01-2019'], ['Kumiko', '30', 'Finance', '7-01-2019'], ['Kumiko', '30', 'Finance', '8-01-2019'], ['Kumiko', '30', 'Finance', '9-01-2019'], ['Kumiko', '30', 'Finance', '10-01-2019'], ['Kumiko', '30', 'Finance', '11-01-2019'], ['Kumiko', '30', 'Finance', '12-01-2019'], ['Kumiko', '30', 'Finance', '13-01-2019'], ['Kumiko', '30', 'Finance', '14-01-2019'], ['Kumiko', '30', 'Finance', '15-01-2019'], ['Kumiko', '30', 'Finance', '16-01-2019'], ['Kumiko', '30', 'Finance', '17-01-2019'], ['Kumiko', '30', 'Finance', '18-01-2019'], ['Kumiko', '30', 'Finance', '19-01-2019'], ['Kumiko', '30', 'Finance', '20-01-2019'], ['Kumiko', '30', 'Finance', '21-01-2019'], ['Kumiko', '30', 'Finance', '22-01-2019'], ['Kumiko', '30', 'Finance', '23-01-2019'], ['Kumiko', '30', 'Finance', '24-01-2019'], ['Kumiko', '30', 'Finance', '25-01-2019'], ['Kumiko', '30', 'Finance', '26-01-2019'], ['Kumiko', '30', 'Finance', '27-01-2019'], ['Kumiko', '30', 'Finance', '28-01-2019'], ['Kumiko', '30', 'Finance', '29-01-2019'], ['Kumiko', '30', 'Finance', '30-01-2019'], ['Kumiko', '30', 'Finance', '31-01-2019']], 'loc_data': [['9608996582', 'Chicago']]})\n",
            "('357919KT', {'dep_data': [['Wendy', '30', 'Finance', '1-01-2019'], ['Wendy', '30', 'Finance', '2-01-2019'], ['Wendy', '30', 'Finance', '3-01-2019'], ['Wendy', '30', 'Finance', '4-01-2019'], ['Wendy', '30', 'Finance', '5-01-2019'], ['Wendy', '30', 'Finance', '6-01-2019'], ['Wendy', '30', 'Finance', '7-01-2019'], ['Wendy', '30', 'Finance', '8-01-2019'], ['Wendy', '30', 'Finance', '9-01-2019'], ['Wendy', '30', 'Finance', '10-01-2019'], ['Wendy', '30', 'Finance', '11-01-2019'], ['Wendy', '30', 'Finance', '12-01-2019'], ['Wendy', '30', 'Finance', '13-01-2019'], ['Wendy', '30', 'Finance', '14-01-2019'], ['Wendy', '30', 'Finance', '15-01-2019'], ['Wendy', '30', 'Finance', '16-01-2019'], ['Wendy', '30', 'Finance', '17-01-2019'], ['Wendy', '30', 'Finance', '18-01-2019'], ['Wendy', '30', 'Finance', '19-01-2019'], ['Wendy', '30', 'Finance', '20-01-2019'], ['Wendy', '30', 'Finance', '21-01-2019'], ['Wendy', '30', 'Finance', '22-01-2019'], ['Wendy', '30', 'Finance', '23-01-2019'], ['Wendy', '30', 'Finance', '24-01-2019'], ['Wendy', '30', 'Finance', '25-01-2019'], ['Wendy', '30', 'Finance', '26-01-2019'], ['Wendy', '30', 'Finance', '27-01-2019'], ['Wendy', '30', 'Finance', '28-01-2019'], ['Wendy', '30', 'Finance', '29-01-2019'], ['Wendy', '30', 'Finance', '30-01-2019'], ['Wendy', '30', 'Finance', '31-01-2019']], 'loc_data': [['9935208129', 'New York']]})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "side_list=list()\n",
        "with open ('./exclude_ids.txt','r') as my_file:\n",
        "  for line in my_file:\n",
        "    side_list.append(line.rstrip())\n",
        "\n",
        "p = beam.Pipeline()\n",
        "\n",
        "# We can pass side inputs to a ParDo transform, which will get passed to its process method.\n",
        "# The first two arguments for the process method would be self and element.\n",
        "\n",
        "class FilterUsingLength(beam.DoFn):\n",
        "  def process(self, element,side_list,lower_bound, upper_bound=float('inf')):\n",
        "    id = element.split(',')[0]\n",
        "    name = element.split(',')[1]\n",
        "    #id=id.decode('utf-8','ignore').encode(\"utf-8\")\n",
        "    element_list= element.split(',')\n",
        "    if (lower_bound <= len(name) <= upper_bound) and id not in side_list:\n",
        "      return [element_list]\n",
        "\n",
        "# using pardo to filter names with length between 3 and 10\n",
        "small_names =(\n",
        "                p\n",
        "                | \"Read from text file\" >> beam.io.ReadFromText('./dept_data.txt')\n",
        "                | \"ParDo with side inputs\" >> beam.ParDo(FilterUsingLength(),side_list,3,10)\n",
        "                | beam.Filter(lambda record: record[3] == 'Accounts')\n",
        "                | beam.Map(lambda record: (record[0]+ \" \" + record[1], 1))\n",
        "                | beam.CombinePerKey(sum)\n",
        "                | 'Write results' >> beam.io.WriteToText('data/output_new_final')\n",
        "             )\n",
        "\n",
        "p.run()\n",
        "\n",
        "!{('head -n 20 data/output_new_final-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZA-Y_HYl_bwT",
        "outputId": "d9a93d0b-25ef-4f28-f4ed-a7966e48f888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('503996WI Edouard', 31)\n",
            "('957149WC Kyle', 31)\n",
            "('241316NX Kumiko', 31)\n",
            "('796656IE Gaston', 31)\n",
            "('718737IX Ayumi', 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "# DoFn function\n",
        "class ProcessWords(beam.DoFn):\n",
        "\n",
        "  def process(self, element, cutoff_length, marker):\n",
        "\n",
        "    name = element.split(',')[1]\n",
        "\n",
        "    if len(name) <= cutoff_length:\n",
        "      return [beam.pvalue.TaggedOutput('Short_Names', name)]\n",
        "\n",
        "    else:\n",
        "      return [beam.pvalue.TaggedOutput('Long_Names', name)]\n",
        "\n",
        "    if name.startswith(marker):\n",
        "      return name\n",
        "\n",
        "\n",
        "p = beam.Pipeline()\n",
        "\n",
        "\n",
        "results = (\n",
        "            p\n",
        "            | beam.io.ReadFromText('./dept_data.txt')\n",
        "\n",
        "            | beam.ParDo(ProcessWords(), cutoff_length=4, marker='A').with_outputs('Short_Names', 'Long_Names', main='Names_A')\n",
        "\n",
        "          )\n",
        "\n",
        "short_collection = results.Short_Names\n",
        "long_collection = results.Long_Names\n",
        "startA_collection = results.Names_A\n",
        "\n",
        "# write to file\n",
        "short_collection | 'Write 1'>> beam.io.WriteToText('short')\n",
        "\n",
        "# write to file\n",
        "long_collection | 'Write 2'>> beam.io.WriteToText('long')\n",
        "\n",
        "# write to file\n",
        "startA_collection | 'Write 3'>> beam.io.WriteToText('start_a')\n",
        "\n",
        "p.run()\n",
        "\n",
        "!{'head -n 5 short-00000-of-00001'}\n",
        "!{'head -n 5 long-00000-of-00001'}\n",
        "!{'head -n 5 start_a-00000-of-00001'}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grLp6TUBHkWO",
        "outputId": "26cb5aae-a809-4242-9023-7698b9ad48d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Itoe\n",
            "Kyle\n",
            "Kyle\n",
            "Olga\n",
            "Kirk\n",
            "Marco\n",
            "Rebekah\n",
            "Edouard\n",
            "Kumiko\n",
            "Gaston\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "p = beam.Pipeline()\n",
        "\n",
        "def calculate_points(element):\n",
        "\n",
        "  customer_id, first_name, last_name, realtionship_id, card_type, max_limit, spent, cash_withdrawn,payment_cleared,payment_date = element.split(',')\n",
        "  #[CT28383,Miyako,Burns,R_7488,Issuers,500,490,38,101,30-01-2018]\n",
        "\n",
        "  spent = int(spent)    # spent = 490\n",
        "  payment_cleared = int(payment_cleared)   #payment_cleared = 101\n",
        "  max_limit = int(max_limit)               # max_limit = 500\n",
        "\n",
        "  key_name = customer_id + ', ' + first_name + ' ' + last_name     # key_name = CT28383,Miyako Burns\n",
        "  defaulter_points = 0\n",
        "\n",
        "  # payment_cleared is less than 70% of spent - give 1 point\n",
        "  if payment_cleared < (spent * 0.7):\n",
        "     defaulter_points += 1                                                # defaulter_points =  1\n",
        "\n",
        "  # spend is = 100% of max limit and any amount of payment is pending\n",
        "  if (spent == max_limit) and (payment_cleared < spent):\n",
        "     defaulter_points += 1                                                # defaulter_points =  2\n",
        "\n",
        "  if (spent == max_limit) and (payment_cleared < (spent*0.7)):\n",
        "     defaulter_points += 1                                                # defaulter_points = 3\n",
        "\n",
        "  return key_name, defaulter_points                                     # {CT28383,Miyako Burns  3}\n",
        "\n",
        "def format_result(sum_pair):\n",
        "  key_name, points = sum_pair\n",
        "  return str(key_name) + ', ' + str(points) + ' fraud_points'\n",
        "\n",
        "\n",
        "\n",
        "card_defaulter = (\n",
        "                  p\n",
        "                  | 'Read credit card data' >> beam.io.ReadFromText('./cards.txt',skip_header_lines=1)\n",
        "                  | 'Calculate defaulter points' >> beam.Map(calculate_points)\n",
        "                  | 'Combine points for defaulters' >> beam.CombinePerKey(sum)                            # key--> CT28383,Miyako Burns   value --> 6\n",
        "                  | 'Filter card defaulters' >> beam.Filter(lambda element: element[1] > 0)\n",
        "                  | 'Format output' >> beam.Map(format_result)                                            # CT28383,Miyako Burns,6 fraud_points\n",
        "                  | 'Write credit card data' >> beam.io.WriteToText('outputs/card_skippers')\n",
        "                  )\n",
        "\n",
        "\n",
        "\n",
        "p.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "u_by3Rs_aCqy",
        "outputId": "3ffd6340-5709-4a8b-bac8-840575a042bc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
              "          var jqueryScript = document.createElement('script');\n",
              "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
              "          jqueryScript.type = 'text/javascript';\n",
              "          jqueryScript.onload = function() {\n",
              "            var datatableScript = document.createElement('script');\n",
              "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
              "            datatableScript.type = 'text/javascript';\n",
              "            datatableScript.onload = function() {\n",
              "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
              "              window.interactive_beam_jquery(document).ready(function($){\n",
              "                \n",
              "              });\n",
              "            }\n",
              "            document.head.appendChild(datatableScript);\n",
              "          };\n",
              "          document.head.appendChild(jqueryScript);\n",
              "        } else {\n",
              "          window.interactive_beam_jquery(document).ready(function($){\n",
              "            \n",
              "          });\n",
              "        }"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<apache_beam.runners.portability.fn_api_runner.fn_runner.RunnerResult at 0x7cd7f93ce230>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "# for datetime manipulation\n",
        "from datetime import datetime\n",
        "\n",
        "p = beam.Pipeline()\n",
        "\n",
        "def calculate_points(element):\n",
        "\n",
        "  customer_id, first_name, last_name, realtionship_id, card_type, max_limit, spent, cash_withdrawn,payment_cleared,payment_date = element.split(',')\n",
        "  #[CT28383,Miyako,Burns,R_7488,Issuers,500,490,38,101,30-01-2018]\n",
        "\n",
        "  spent = int(spent)    # spent = 490\n",
        "  payment_cleared = int(payment_cleared)   #payment_cleared = 101\n",
        "  max_limit = int(max_limit)               # max_limit = 500\n",
        "\n",
        "  key_name = customer_id + ', ' + first_name + ' ' + last_name     # key_name = CT28383,Miyako Burns\n",
        "  defaulter_points = 0\n",
        "\n",
        "  # payment_cleared is less than 70% of spent - give 1 point\n",
        "  if payment_cleared < (spent * 0.7):\n",
        "     defaulter_points += 1                                                # defaulter_points =  1\n",
        "\n",
        "  # spend is = 100% of max limit and any amount of payment is pending\n",
        "  if (spent == max_limit) and (payment_cleared < spent):\n",
        "     defaulter_points += 1                                                # defaulter_points =  2\n",
        "\n",
        "  if (spent == max_limit) and (payment_cleared < (spent*0.7)):\n",
        "     defaulter_points += 1                                                # defaulter_points = 3\n",
        "\n",
        "  return key_name, defaulter_points                                     # {CT28383,Miyako Burns  3}\n",
        "\n",
        "def format_result(sum_pair):\n",
        "  key_name, points = sum_pair\n",
        "  return str(key_name) + ', ' + str(points) + ' fraud_points'\n",
        "\n",
        "def calculate_late_payment(elements):               # [CT88330,Humberto,Banks,Serviceman,LN_1559,Medical Loan,26-01-2018,2000,30-01-2018]\n",
        "\n",
        "  due_date = datetime.strptime(elements[6].rstrip().lstrip(), '%d-%m-%Y')           # due_date = 26-01-2018\n",
        "  payment_date = datetime.strptime(elements[8].rstrip().lstrip(), '%d-%m-%Y')       # payment_date = 30-01-2018\n",
        "\n",
        "  if payment_date <= due_date:\n",
        "    elements.append('0')\n",
        "  else:\n",
        "    elements.append('1')                           # [CT88330,Humberto,Banks,Serviceman,LN_1559,Medical Loan,26-01-2018,2000,30-01-2018,1]\n",
        "\n",
        "  return elements\n",
        "\n",
        "def format_output(sum_pair):\n",
        "  key_name, miss_months = sum_pair\n",
        "  return str(key_name) + ', ' + str(miss_months) + ' missed'\n",
        "\n",
        "def calculate_month(input_list):        #input  [CT88330,Humberto,Banks,Serviceman,LN_1559,Medical Loan,26-01-2018, 2000, 30-01-2018]\n",
        "\n",
        "  # Convert payment_date to datetime and extract month of payment\n",
        "  payment_date = datetime.strptime(input_list[8].rstrip().lstrip(), '%d-%m-%Y')  # payment_date = 30-01-2018\n",
        "  input_list.append(str(payment_date.month))                                     # [CT88330,Humberto,Banks,Serviceman,LN_1559,Medical Loan,26-01-2018, 2000, 30-01-2018, 01]\n",
        "\n",
        "  return input_list\n",
        "\n",
        "def calculate_personal_loan_defaulter(input):       #input key -> CT68554,Ronald Chiki   value --> [01,05,06,07,08,09,10,11,12]\n",
        "    max_allowed_missed_months = 4\n",
        "    max_allowed_consecutive_missing = 2\n",
        "\n",
        "    name, months_list = input                                   # [CT68554,Ronald,Chiki,Serviceman,LN_8460,Personal Loan,25-01-2018,50000,25-01-2018]\n",
        "\n",
        "    months_list.sort()\n",
        "    sorted_months = months_list                                 # sorted_months = [01,05,06,07,08,09,10,11,12]\n",
        "    total_payments = len(sorted_months)                         # total_payments = 10\n",
        "\n",
        "    missed_payments = 12 - total_payments                       # missed_payments = 2\n",
        "\n",
        "    if missed_payments > max_allowed_missed_months:             # false\n",
        "       return name, missed_payments                             #  N/A\n",
        "\n",
        "    consecutive_missed_months = 0\n",
        "\n",
        "    temp = sorted_months[0] - 1                                 # temp = 0\n",
        "    if temp > consecutive_missed_months:                        # false\n",
        "        consecutive_missed_months = temp                        #NA\n",
        "\n",
        "    temp = 12 - sorted_months[total_payments-1]\n",
        "    if temp > consecutive_missed_months:\n",
        "        consecutive_missed_months = temp                        # temp = 0\n",
        "\n",
        "    for i in range(1, len(sorted_months)):                      # [01,05,06,07,08,09,10,11,12]\n",
        "        temp = sorted_months[i] - sorted_months[i-1] -1         # temp = 5-1-1 = 3\n",
        "        if temp > consecutive_missed_months:\n",
        "            consecutive_missed_months = temp                    # consecutive_missed_months = 3\n",
        "\n",
        "    if consecutive_missed_months > max_allowed_consecutive_missing:\n",
        "       return name, consecutive_missed_months                   # CT68554,Ronald Chiki   3\n",
        "\n",
        "    return name, 0\n",
        "\n",
        "def return_tuple(element):\n",
        "  thisTuple=element.split(',')\n",
        "  return (thisTuple[0],thisTuple[1:])\n",
        "\n",
        "card_defaulter = (\n",
        "                  p\n",
        "                  | 'Read credit card data' >> beam.io.ReadFromText('cards.txt',skip_header_lines=1)\n",
        "                  | 'Calculate defaulter points' >> beam.Map(calculate_points)\n",
        "                  | 'Combine points for defaulters' >> beam.CombinePerKey(sum)                            # key--> CT28383,Miyako Burns   value --> 6\n",
        "                  | 'Filter card defaulters' >> beam.Filter(lambda element: element[1] > 0)\n",
        "                  | 'Format output' >> beam.Map(format_result)                                            # CT28383,Miyako Burns,6 fraud_points\n",
        "                 # | 'Write credit card data' >> beam.io.WriteToText('outputs/card_skippers')\n",
        "                  | 'tuple ' >> beam.Map(return_tuple)\n",
        "                  )\n",
        "\n",
        "medical_loan_defaulter = (\n",
        "                            p\n",
        "                            |  beam.io.ReadFromText('loan.txt',skip_header_lines=1)   # 1stRow--> CT88330,Humberto,Banks,Serviceman,LN_1559,Medical Loan,26-01-2018, 2000, 30-01-2018\n",
        "                            | 'Split Row' >> beam.Map(lambda row : row.split(','))\n",
        "                            | 'Filter medical loan' >> beam.Filter(lambda element : (element[5]).rstrip().lstrip() == 'Medical Loan')\n",
        "                            | 'Calculate late payment' >> beam.Map(calculate_late_payment)\n",
        "                            | 'Make key value pairs' >> beam.Map(lambda elements: (elements[0] + ', ' + elements[1]+' '+elements[2], int(elements[9])) )\n",
        "                            | 'Group medical loan based on month' >> beam.CombinePerKey(sum)                       # key--> (CT88330,Humberto Banks)  value --> 7\n",
        "                            | 'Check for medical loan defaulter' >> beam.Filter(lambda element: element[1] >= 3)\n",
        "                            | 'Format medical loan output' >> beam.Map(format_output)      # CT88330,Humberto Banks,7 missed\n",
        "                         )\n",
        "\n",
        "personal_loan_defaulter = (\n",
        "                            p\n",
        "                            | 'Read' >> beam.io.ReadFromText('loan.txt',skip_header_lines=1)\n",
        "                            | 'Split' >> beam.Map(lambda row : row.split(','))\n",
        "                            | 'Filter personal loan' >> beam.Filter(lambda element : (element[5]).rstrip().lstrip() == 'Personal Loan')\n",
        "                            | 'Split and Append New Month Column' >> beam.Map(calculate_month)\n",
        "                            | 'Make key value pairs loan' >> beam.Map(lambda elements: (elements[0] + ', ' + elements[1]+' '+elements[2], int(elements[9])) )\n",
        "                            | 'Group personal loan based on month' >> beam.GroupByKey()                                  # CT68554,Ronald Chiki [01,05,06,07,08,09,10,11,12]\n",
        "                            | 'Check for personal loan defaulter' >> beam.Map(calculate_personal_loan_defaulter)          # CT68554,Ronald Chiki   3\n",
        "                            | 'Filter only personal loan defaulters' >> beam.Filter(lambda element: element[1] > 0)\n",
        "                            | 'Format personal loan output' >> beam.Map(format_output)        # CT68554,Ronald Chiki,3 missed\n",
        "                          )\n",
        "\n",
        "final_loan_defaulters = (\n",
        "                          ( personal_loan_defaulter, medical_loan_defaulter )\n",
        "                          | 'Combine all defaulters' >> beam.Flatten()\n",
        "                          #| 'Write all defaulters to text file' >> beam.io.WriteToText('outputs/loan_defaulters')\n",
        "                          | 'tuple for loan' >> beam.Map(return_tuple)\n",
        "                        )\n",
        "\n",
        "both_defaulters =  (\n",
        "                    {'card_defaulter': card_defaulter, 'loan_defaulter': final_loan_defaulters}\n",
        "                    | beam.CoGroupByKey()\n",
        "                    |'Write p3 results' >> beam.io.WriteToText('outputs/both')\n",
        "                   )\n",
        "\n",
        "\n",
        "p.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXi_lfP6WUPa",
        "outputId": "7e667a86-c166-4126-e9f7-6919f6d74a69"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<apache_beam.runners.portability.fn_api_runner.fn_runner.RunnerResult at 0x7cd7f82209d0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}